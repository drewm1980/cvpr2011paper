\documentclass[10pt,twocolumn,letterpaper]{article}
% ICCV Rules:
% 8 page limit (6 free + 2 paid)
% 10MB pdf file size limit
% 30MB supplementary materials file size limit (PDF or ZIP)

\usepackage{cvpr}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}

% Needed for getting formulas in dia diagrams.
\usepackage{pstricks}
\usepackage{tikz}

\usepackage{mydefs}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{257} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%\title{Accelerating $\ell_1$-Minimization Using Many-Core CPUs/GPUs \\ and Application to Face Recognition
%\title{Efficient Parallelization of Sparse Representation for Face Recognition}
%\thanks{Corresponding author: . This work was partially supported by ARO MURI W911NF-06-1-0076.}}
%\title{Many-Core Parallelization of $\ell_1$-Minimization for Face Recognition
\title{Parallelization of Fast $\ell_1$-Minimization for Face Recognition}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\small\url{http://www.author.org/~second}}
}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction} 
$\ell_1$-minimization ($\ell_1$-min) has received much attention in recent
years due to important applications in compressive sensing
\cite{BrucksteinA2007} and sparse representation \cite{WrightJ2010-PIEEE}.  
$\ell_1$-min refers to finding the minimum $\ell_1$-norm solution to an
underdetermined linear system $\bb=A\xx$:
\begin{equation}
\min \|\xx\|_1\quad \mbox{ subj. to }\quad \bb = A\xx.
\label{eq:l1min}
\end{equation}
It is now well established that, under certain conditions
\cite{CandesE2005-IT_1,DonohoD2004}, the minimum $\ell_1$-norm solution is also
the \emph{sparsest} solution to the system \eqref{eq:l1min}.

Among its many applications, $\ell_1$-min has been recently used to reformulate
image-based face recognition as a sparse representation problem
\cite{WrightJ2009-PAMI}.  If we stack the training images of $K$ subject
classes $(A_1\in\Re^{d\times n_1}, \cdots, A_K\in\Re^{d\times n_K})$ into the
columns of a matrix $A = [A_1, \cdots, A_K]\in\Re^{d\times n}$, given a new
query image in the same vector form $\bb\in\Re^d$, \emph{sparsity-based
classification} (SBC) solves the following minimization problem:
\begin{equation}
\min_{\xx, \ee} \| \xx \|_1 + \|\ee\|_1 \quad \subj \quad \bb = A \xx + \ee.
\label{eq:l1min_denoise}
\end{equation}
If the sparsest solutions for $\x$ and $\e$ are recovered, $\ee$ provides a
means to compensate for pixels that are corrupted due to occlusion of some part of the query
image, and the dominant nonzero coefficients of $\xx$ reveal the membership of
$\bb$ based on the training image labels associated with $A$.  Since $A$ ideally
contains many images per user taken under different illuminations, 
$A\x$ also acts as a linear illumination model for the test image $\bb$.

In this paper, we study parallelization of $\ell$-min on many-core CPU and GPU
architectures. Although $\ell_1$-min \eqref{eq:l1min} is a convex
program, conventional algorithms such as interior-point methods
\cite{ChenS2001-SIAM,TibshiraniR1996} are known to suffer from poor scalability
for large-scale applications such as face recognition. Recently, a number of
accelerated algorithms have been proposed that explicitly take advantage of
the special structure of $\ell_1$-min problems
\cite{LorisI2009,YangA2010-ICIP}. We investigate parallelization of a
state-of-the-art $\ell_1$-min solution based on the classical framework of
\emph{augmented Lagrangian methods} (ALM) \cite{BertsekasD2003,YangA2010-ICIP}.

While solving \eqref{eq:l1min_denoise} can achieve exceptional
recognition accuracy on public databases, its success also relies on the assumption
that the query image is well aligned with the training images. In
\cite{WagnerA2009-CVPR}, the framework of SBC was extended to iteratively align
a query image to each subject class individually. The face alignment algorithm solves
the following minimization problem:
\begin{equation}
\hat{\tau}_i = \arg\min_{\xx, \ee, \tau_i} \|\ee\|_1\quad \mbox{subj. to}\quad \tilde\bb\circ\tau_i = A_i\xx + \ee,
\label{eq:l1min_alignment}
\end{equation}
where $\tau_i\in T$ is in a finite-dimensional group $T$ of transformations
(e.g., affine or homography) acting on the image domain.  The sequence
$\tau_i$ converges to a transformation that aligns the test image $\tilde\bb$
with the training images from the $i$-th class $A_i$. In other words, the algorithm 
extends Lucas-Kanade iterative alignment \cite{LucasB1981} to use the
$\ell_1$-norm as a robust error function, while simultaneously estimating
the coefficients of the linear illumination model $A\x$.  

Numerically, the original solution \cite{WagnerA2009-CVPR} was implemented based 
on standard interior-point methods, and its speed has been since improved by ALM in a more recent paper
\cite{WagnerA2011-PAMI}. However, due the high per-class cost
of the alignment step, the recognition system still fails to achieve \emph{real-time recognition} against datasets of hundreds or thousands of users.
In this paper, in addition to accelerating the generic
$\ell_1$-min objectives \eqref{eq:l1min} and \eqref{eq:l1min_denoise}, we 
also discuss how to properly accelerate the iterative face alignment step
\eqref{eq:l1min_alignment} on multi-core CPUs and GPUs.

In this paper we will clearly distinguish between algorithm {\em parallelism},
which is a property of an algorithm, and hardware {\em concurrency}, which is a
property of a hardware architecture. Parallelism in an algorithm provides the
opportunity to perform computations concurrently on hardware.  Algorithms often
exhibit multiple levels of parallelism, and hardware often provides multiple
levels of concurrency.  In these terms, the primary contribution of this paper
will be to determine the optimal mappings between the parallelism available in
$\ell_1$-min and the concurrency available in multi-core CPU and GPU
architectures.

Finally, we present extensive benchmark to compare the performance of the
generic ALM $\ell_1$-min algorithm and the corresponding face recognition
routines on massively parallel CPUs and GPUs.  To this end, a new face
recognition system for security and access control applications has been fully implemented on a state-of-the-art
workstation, as shown in Figure \ref{fig:pipeline}. 
Since the parallelization of any algorithm
is highly dependent on the target hardware, we will give a brief overview of
the architecture of our test system in detail in Section
\ref{sec:concurrency}.
Finally, note that for applications targeting thousands of users or more, it would likely be
necessary to distribute the work over a network of computers, while this paper
focuses on the parallelism available in a single (shared memory) machine.
The C++ source code of our implementations, which target Nvidia's CUDA GPU architecture,
will be made available for peer evaluation.
\begin{figure}
\centering
{\tiny \input{figures/pipeline_simplified.tex}}
\caption{The face recognition pipeline.}
\label{fig:pipeline}
\end{figure}

\subsection{Literature Review} 
We first briefly review the literature of $\ell_1$-min and existing
works on its parallelization.  Traditionally, $\ell_1$-min (or the so-called
basis pursuit (BP)) has been formulated as a linear program
\cite{ChenS2001-SIAM}. Several variations of the solution are also well known
in optimization, including a noisy approximation via quadratic programming
called the LASSO \cite{TibshiraniR1996} and truncated Newton interior-point
method (TNIPM) \cite{KimS2007}.

One of the drawbacks of most interior-point methods for $\ell_1$-min (including
BP, LASSO, and TNIPM) is that they require the solution sequence to follow an
interior path via gradient descent or conjugate gradient methods, which are computationally expensive.
To mitigate these issues, an approach called \emph{Homotopy} has been recently studied to accelerate the
speed of $\ell_1$-min. Homotopy methods for recovering sparse signals were
first studied in the context of LASSO \cite{OsborneM2000}, which inspired a
solution to the \emph{forward stagewise linear regression} problem called LARS
\cite{EfronB2004}, and eventually led to the corresponding Homotopy algorithms
for BP in \cite{MalioutovD2005,DonohoD2006}.

Although Homotopy can be shown to exactly estimate BP when the solution is
sufficiently sparse \cite{DonohoD2006}, the algorithm still involves 
computationally expensive operations such as matrix-matrix multiplication and 
the solution of least-squares problems with varying $A$ matrices. 
In Section \ref{sec:ALM}, we contend
that ALM is a better choice for implementation on many-core CPUs and GPUs. The ALM algorithm belongs to a
category of approximate $\ell_1$-min solutions called \emph{iterative
shrinkage-thresholding} (IST) methods \cite{WrightS2008,BeckA2009}.
IST algorithms mainly involve elementary operations such as vector 
algebra and matrix-vector multiplication. Therefore,
when the dimension of the problem becomes high, IST-type algorithms are
particularly suitable for hardware systems with a high degree of concurrency. In
\cite{YangA2010-ICIP}, the authors further showed that, compared to other
IST-type algorithms, ALM is able to achieve good estimation accuracy comparable
to BP and Homotopy, while at the same time significantly improve the speed
compared to these exact $\ell_1$-min solutions. Due to these facts, in this
paper, we choose ALM as the core algorithm to implement the complete parallel
face recognition pipeline.

In terms of the past works in parallel $\ell_1$-min, the literature has been
limited, to the best of our knowledge. In \cite{BorghiA2010}, Borghi et al.
developed a special proximal gradient $\ell_1$-min algorithm based on
Moreau-Yosida regularization. In \cite{MurphyM2010}, Murphy et al. presented
parallel implementation of the l1-SPIRIT MRI reconstruction algorithm on NVidia
CUDA programmable GPU architecture, which is also used in this paper.

\section{Augmented Lagrange Multiplier Method}
\label{sec:ALM}

For the completeness of the paper, in this section, we briefly describe the ALM
algorithm for $\ell_1$-min \eqref{eq:l1min} \cite{YangA2010-ICIP} and analyze
its complexity. Parallelization of ALM for face alignment and recognition will
be discussed in Section \ref{sec:alignment} and Section \ref{sec:recognition},
respectively.

Lagrange multiplier methods have been frequently used in convex programming to
eliminate equality constraints via adding a significant penalty term to the
cost function for infeasible points. ALM methods differ from other
penalty-based approaches by simultaneously estimating the optimal solution and
Lagrange multipliers in an iterative fashion.  For $\ell_1$-min, the augmented
Lagrangian is defined as: \begin{equation} L_\mu(\xx,\yy) = \|\xx\|_1 +
\yy^T(\bb - A\xx - \ee) + \frac{\mu}{2} \| \bb-A\xx-\ee \|_2^2, \end{equation}
where $\mu > 0$ is a constant that penalizes infeasibility and $\yy$ is a
vector of lagrange multipliers.

In Lagrange Multiplier Theory, if there exists a Lagrangian $y^*$ that
satisfies the second-order sufficiency conditions for optimality
\cite{BertsekasD2003}, then for a sufficiently large $\mu$, the optimal
$\ell_1$-min solution also minimizes
\begin{equation}
\xx^* = \arg \min L_\mu(\xx,\yy^*).
\label{eq:optimal-ALM}
\end{equation}

In practice, the optimal values for the triplet $(\xx^*, \yy^*, \mu)$ are all
unknown. Furthermore, it has been observed that solving
\eqref{eq:optimal-ALM} with a large initial value of $\mu$ tends to lead to
slower convergence speed \cite{WrightS2008,YangA2010-ICIP}. In
\cite{BertsekasD2003,YangJ2009}, an alternating procedure has been shown to
iteratively update $\xx$ and $\yy$:
\begin{equation}
\left \{
\begin{array}{lll}
\xx_{k+1} & = & \arg\min_{\xx} \, L_{\mu_k} (\xx,\yy_k)\\
\yy_{k+1} & = & \yy_k + \mu_k (\bb - A\xx_{k+1}) \\
\end{array}
\right . ,
\label{eq:alm}
\end{equation}
where $\mu_{k}\rightarrow \infty$ is a monotonically increasing positive sequence.
The iteration terminates when the estimates $(\xx, \yy)$ converge.

Note that in the iterative procedure \eqref{eq:alm}, the second
step only involves vector algebra and matrix-vector multiplication. Therefore,
the procedure is computationally efficient only if it is easier to minimize the
augmented Lagrangian $L_{\mu_k} (\xx,\yy_k)$ compared to solving the original problem
\eqref{eq:l1min} directly. In fact, this problem can be solved element-wise
iteratively by a soft-thresholding algorithm \cite{WrightS2008,BeckA2009},
whose time complexity is bounded by $O(n^2)$ and can be easily parallelized.
Algorithm \ref{alg:alm} summarizes the generic ALM $\ell_1$-min algorithm.\footnote{For conciseness, we
only present the ALM algorithm in the primal domain. There also
exist implementations in the dual domain \cite{YangJ2009,YangA2010-ICIP}.}

 \begin{algorithm}[h]
\caption{Augmented Lagrangian Method (ALM)}
{\bf INPUT:} $\bb \in \Re^m$, $A=[A_1,\cdots, A_K] \in \Re^{m \times n}$, $\tau\leftarrow \max\mbox{eig}(A^TA)$, and constant $\rho>1$.
\begin{algorithmic}[1]
\WHILE{not converged ($k = 1,2,\ldots$)} 
\STATE $t_1 \leftarrow 1$, $\zz_1 \leftarrow \xx_k$, $\uu_1 \leftarrow \xx_k$ 
\WHILE{not converged ($l = 1,2,\ldots$)} 
\STATE $\uu_{l+1}  \leftarrow \mbox{shrink}(\zz_l - \frac{1}{\tau}A^T(A\zz_l - \bb - \frac{1}{\mu_k}\yy_k), \frac{1}{\mu_k\tau})$
\STATE $t_{l+1} \leftarrow \frac{1}{2}( 1 + \sqrt{1+4t_l^2})$
\STATE $\zz_{l+1} \leftarrow \uu_{l+1}+ \frac{t_l - 1}{t_{l+1}}(\uu_{l+1} - \uu_l)$ 
\ENDWHILE 
\STATE $\xx_{k+1} \leftarrow \uu_{l+1}$ 
\STATE $\yy_{k+1} \leftarrow \yy_k + \mu_k (\bb - A\xx_{k+1})$ 
\STATE $\mu_{k+1} \leftarrow \rho\cdot\mu_k$ 
\ENDWHILE 
\end{algorithmic}

{\bf OUTPUT:} $\xx^* \leftarrow \xx_k$.
\label{alg:alm}
\end{algorithm}

\section{Hardware Concurrency} \label{sec:concurrency}

In this section we discuss the levels of concurrency available in the hardware
architectures considered in this paper, as well as other aspects of the
hardware that are important for performance.  In particular, since on-chip
memory (cache) is often orders of magnitude faster than off-chip memory, the
sizes and speeds of on-chip memory (cache) have big performance implications.
We give a brief overview of the caches that are available in our target
architectures, and defer discussion of their performance implications to
sections \ref{sec:alignment_implementation_cpu} and
\ref{sec:alignment_implementation_gpu}.

We will primarily be comparing CPU and GPU architectures on a per-board basis:
for CPU implementation we make use of all the cores in as many CPUs are present
on one motherboard, and for GPU implementation we make use of a single
many-core Nvidia GPU chip on a PCI card. GPUs are commonly shipped with a
single GPU chip per PCI-express card.  Note that most engineering  workstations
are capable of supporting several GPU cards.  Unless otherwise specified, all
implementations utilize single precision floating point datatypes.  

\subsection{CPU Hardware Concurrency} 
\label{sec:CPU-concurrency}
The main defining characteristics of contemporary multi-core CPU architectures
are that they have two levels of concurrency, relatively large amounts of
on-chip cache, and relatively high clock speeds.

The baseline architecture for our experiments is a Linux workstation with two
quad-core Intel Nehalem E5530 processors clocked at 2.4 GHz.  Each processor
has its own memory interface, and is directly interfaced to half of the RAM
installed in the machine.  The amount of RAM installed exceeds the amount used
by the algorithms, and is not an important performance consideration.  

\begin{figure}
\begin{center}
\includegraphics[width=3.5in]{figures/caches}
\end{center}
\caption{Scale Drawing of the CPU and GPU cache sizes}
\label{fig:caches}
\end{figure}

% CPU CACHE DISCUSSION
%2MB = 2**21 bytes
%64*64*4 = 2**14 bytes = 16384 bytes per image
%... so can fit 2**7  = 128 images.
Each core has a private 32\,KB L1 data cache and a private 256\,KB L2 cache.
%16MB = 2**24 bytes = 16777216 bytes
%64*64*4 = 2**14 bytes = 16384 bytes per image
%... so can fit 2**10  = 1024 images.
Each processor further has 8\,MB of L3 cache that is shared by the four cores.
Overall, the algorithm has approximately 16\,MB of L3 cache
available for a dual-processor configuration.  

For floating-point instructions, each core also has a vector processing unit
(SSE) capable of performing the same arithmetic operation on four single-precision 
floating point values simultaneously \footnote{By the publication date, the vector
units of typical high-end CPUS will double.}.  

There are thus two important levels of concurrency that need to be exploited to
efficiently use a modern CPU: {\em core-level} concurrency and {\em sse-level}
concurrency. 

\subsection{GPU Hardware Concurrency}
The main defining characteristics of contemporary multi-core GPU architectures
are that they have two (much wider) levels of concurrency, relatively small amounts of
on-chip cache, and relatively low clock speeds.  Whereas most of the transistors on
a typical CPU are dedicated either to cache or to hardware that enables higher
clock speeds (such as branch prediction, out-of-order instruction execution, etc...),
most of the transistors on a GPU are dedicated to arithmetic logic units.
An explanation of the CUDA programming model at a useful level of completeness would take 
more space than is available here, so we will instead frame our discussion in terms of 
hardware capabilities\footnote{A discussion of the CUDA programming model can be found
in the NVIDIA C for CUDA Programming Guide}.

Our target GPU architecture is the Nvidia Fermi architecture (\eg, the GTX 480
GPU used in this paper).  CUDA based GPUs are comprised of several
\emph{streaming multiprocessors} (SMP), each of which is roughly analogous to a CPU core.
For Fermi architecture GPU's, there are up to 16 SMP's, and each SMP is capable of executing up to 64 single precision 
floating point operations
concurrently\footnote{In CUDA terms, the warp width is 32, and the floating pipeline can
issue up to two warps simultaneously}. 

% GPU CACHE DISCUSSION
% for L2 cache
% 768KB = 786432 bytes
% 64*64*4 = 2**14 bytes = 16384 bytes per image
% 48 images
Each SMP contains its own L1-cache, which is divided between hardware managed
and software managed ("shared") memory.  Additionally, all SMPs share a common
L2-cache.  For our system, each SMP has 64 KB of L1 cache, and all SMP's share
768KB of L2 cache. 
%The cache hierarchy 
The relatively small amount
of cache (1/23 as much as CPU L3) on the GPU is balanced by a significantly
higher bandwidth between the processor chip and off-chip memory (DRAM) compared
to the CPU.  A scale drawing of the caches available on the GPU can be seen in Figure \ref{fig:caches}.
The GPU has its own memory system, and any data the GPU uses must first be
transferred from CPU DRAM to GPU DRAM over PCI-Express.  For our application,
this transfer overhead can be amortized over a large amount of computation and is
not a major concern.

While the programming model for
the SIMD units of each SMP is somewhat more flexible than on the CPU,
leveraging the flexibility typically comes at the cost of reduced parallelism
\footnote{In CUDA terms, code branches that cause warp divergence usually
result in serialization of the different groups of threads}.  Therefore, for
the purposes of comparing hardware architectures, the SIMD units on the GPU are
roughly analogous to the CPU SIMD units. Thus, in summary, the GPU hardware 
provides two levels of concurrency: {\em SMP-level}
concurrency and {\em warp-level} concurrency.  Note that the GPU provides more
concurrency than a cpu at both levels (14 SMP's vs. 8 cores) and (64 wide vs. 4 wide SIMD
units).

\section{Parallelism in the Face Alignment Stage}
\label{sec:alignment}

Given the hardware concurrency in modern parallel computing architectures, next,
we shall study the optimal mappings of the algorithm parallelism in face recognition to the multi-core CPUs and GPUs, respectively.
In this section, we first focus on the face alignment stage (see Figure \ref{fig:pipeline}).

Face alignment \eqref{eq:l1min_alignment} estimates an image transformation $\tau$ that rectifies the query image $\tilde{\bb}$ with possible pose variation
w.r.t. each training class $A_i$, which leads to the minimal sparsity in error $\ee$ after the alignment. Note that directly solving \eqref{eq:l1min_alignment} would require
simultaneously estimating $\xx$, $\ee$, and $\tau$ typically on a nonlinear manifold, which renders the problem extremely difficult to solve exactly with many local minima.

To mitigate the problem, if the system can obtain to a good initial guess of the transformation $\tau$ (\eg, provided by a good face detector),
the optimal solution for $\tau$ can be sought iteratively by a sequence of linearized approximations $j=1, 2, \cdots$ as follows:
\begin{equation}
\min_{\xx, \ee, \Delta \tau_j}\|\ee\|_1\quad \subj\quad \tilde\bb\circ\tau_j +  J_j\Delta \tau_j = A_i\xx + \ee,
\end{equation}
where $J_j = \nabla_{\tau_j}(\tilde\bb\circ\tau_j)$ is the Jacobian, and
$\Delta \tau$ is a step update to $\tau$. Denote $\bb_j =
\tilde\bb\circ\tau_j$, $B_j = [A_i, -J_j]$ and $\ww^T = [\xx^T, \Delta
\tau^T]$, then the update $\Delta \tau$ can be computed by solving the
following linear program:
\begin{equation}
\min_{\ww, \ee}\|\ee\|_1\quad \subj\quad \bb_j = B_j\ww + \ee.
\label{eq:alignment-linearization}
\end{equation}
%If the warping $\tau$ is constrained to be a similarity transform,
%it can be paramaterized in such a way that the computation of $\frac{\partial \bb}{\partial \tau_p}$
%reduces to several linear vector operations involving pixel coordinates $(u,v)$, $\tau \in \Re^4$,
%and the image gradients $\f_x, \f_y \in \Re^M$, the latter of which only has to be computed
%once per test image.  

The per-class alignment algorithm via ALM is summarized in Algorithm
\ref{alg:alignment_alm}. In terms of computational complexity, one significant
term is the computation of the matrix psudo-inverse $B_j^\dagger$, whose
computational cost is bounded by $O(n_i^2m + n_i^3)$. For per-class face
alignment, since both $A_i$ and $J_{j}$ are very tall matrices (\ie, $m\gg n_i$
for all $i=1, \cdots, K$ classes), we find that the computation time is still
dominated by the iterative computation of matrix-vector multiplications in the
inner loop.
\begin{algorithm}[ht!]
\caption{\bf (Face Alignment via ALM)} \label{alg:alignment_alm}
{\bf Input:} $\bb$, $A_i$, $\x_0 = \mathbf{0}$, $\tau_0$, and $J_0$.
\begin{algorithmic}[1]
\WHILE{not converged ($j = 1,2,\ldots$)}
\STATE Update $\bb_j \leftarrow \frac{\bb\circ \tau_{j-1}}{\|\bb\circ \tau_{j-1}\|}$; $B_j= [A_i, -J_{j-1}]$ and corresponding $(B_j^\dagger)^T$
\STATE Initialize $\ww_0 = \mathbf 0$, $\yy_0 = \mathbf 0$
\WHILE{not converged ($k = 1,2,\ldots$)}
\STATE $\uu_0\leftarrow \ww_{k-1}$; $\zz_0\leftarrow \ee_{k-1}$
\WHILE{not converged ($l = 1,2,\ldots$)}
\STATE $\zz_l \leftarrow \mbox{shrink}\left(\bb_j - B_j\uu_{l-1} + \frac{\yy_{k}}{\mu_{k-1}}, \frac{1}{\mu_{k-1}}\right)$
\STATE $\uu_l \leftarrow (B_j^\dagger)^T \left(\bb_j - \zz_{l} + \frac{\yy_{k-1}}{\mu_{k-1}} \right) $
\ENDWHILE
\STATE $\ww_k \leftarrow \uu_l$; $\ee_k \leftarrow \zz_l$
\STATE $\yy_{k} \leftarrow \yy_{k-1} + \mu_{k-1} (\bb_j - B_j\ww_{k} - \e_{k})$
\STATE $\mu_{k} \leftarrow \rho\mu_{k-1}$
\ENDWHILE
\STATE Update $\ee_j$, $\tau_j$, and $J_j$
\ENDWHILE
\end{algorithmic}
{\bf Output:} $\tau^*\leftarrow \tau_j, \e^*\leftarrow \ee_j$
\end{algorithm}

In summary, the alignment stage essentially contains two levels of available
parallelism. At the higher level, there are per-class alignment problems
that are solved independently, or \emph{problem-level parallelism}.  At a lower
level, the first-order linear algebraic operations exhibit parallelism within
image operations, \ie, at the pixel level.  We call this \emph{pixel-level
parallelism}.  In the following two sections will will discuss how to
exploit these two levels of parallelism on different CPU and GPU architectures.

\subsection{CPU Implementation} 
\label{sec:alignment_implementation_cpu}

Optimal implementation of Algorithm \ref{alg:alignment_alm} on a multicore CPU must take 
into account the properties of the cache hierarchy. In general, cache that is 
closer to the core (i.e. L1 cache) has higher bandwidth, but smaller size compared to cache
that is farther from the core (L3 cache).  For reference, Figure \ref{fig:caches} 
shows the the sizes of L2 and L3 caches of the Intel E5530 CPU, 
which is a representative example of a modern multicore CPU. 

For the E5530, the L2 cache in each core is only able to store about 16 images.
For image alignment problem, the number of training images per class is
typically larger than 16.  Therefore, if eight instances of
\eqref{eq:alignment-linearization} were be executed in parallel on the eight
cores, there would be an insufficient amount of L1 or L2 cache to hold $A$ and
$A^\dagger$ during the iterative process to execute Algorithm
\ref{alg:alignment_alm}. Therefore, the bottleneck in this strategy becomes the
bandwidth between the L2 cache and L3 cache. The implementation of some
previous works \cite{WagnerA2009-CVPR,WagnerA2011-PAMI} is indeed based on this
mapping strategy.

In this paper, we show that for CPU parallelism of face alignment, a higher
performance implementation can be achieved by {\em not} taking advantage of
problem-level parallelism at all, and instead map the pixel-level parallelism
onto both the core-level and the sse-level concurrency provided by the cpu.  In
other words, the proposed CPU implementation performs alignment of a query
image against the training images of a single user at a time using all cores on
both processors.
This mapping of the problem is illustrated in Figure \ref{fig:alignment_mapping_cpu}.

More specifically, in our implementation, most of the operations take
advantage of Intel Math Kernel Library (MKL), a commercial implementation of
the standard Basic Linear Algebra Subprograms (BLAS). For the image resampling
step, we make use of the Intel Integrated Performance Primitives (IPP) library.
Both MKL and IPP are optimized for Intel multicore CPUs, and are able to
automatically utilize both core-level and vector-level concurrency. For other
operations that are not optimized by Intel in-house libraries, we achieve both
vector-level and core-level concurrency through a combination of the Open MP
API \cite{dagum2002openmp} and the automatic vectorization facilities of the
Intel C++ compiler (ICC) \cite{dulong1999overview}. We shall compare the
performance of the two parallelism methods in Section \ref{sec:experiment}.

\subsection{GPU Implementation}
\label{sec:alignment_implementation_gpu}
While on the CPU, algorithm performance is highly dependent on effective
cache usage, on the GPU cache is relatively unimportant for ALM based
$\ell_1$-min.  As can be seen in Figure \ref{fig:caches}, the GPU has 
a very small amount of cache compared to the CPU.  While it might be possible
to fit a single instance of the alignment problem (with a slightly reduced problem size)
into L2 cache, this would not be an efficient use of the GPU's resources.
The strength of the GPU's memory architecture for our purposes it's ability to sustain a very
high bandwidth to DRAM.  A higher overall bandwidth can be achieved if 
many memory transactions are performed simultaneously, than can be achieved
with a large number of smaller transactions that are just large enough to fill
the L2 cache.

Therefore, whereas on the CPU we advocate performing alignment for one training
class at a time, on the GPU we advocate performing alignment for many classes
concurrently.  \footnote{In CUDA terminology, Our proposed alignment stage
implementation consists of a single kernel that performs alignment for all
subject classes.  Each instance of the alignment problem is assigned its own
thread block, and the GPU hardware schedules as many thread blocks to run
concurrently as the hardware will allow.} The number of subject classes that
actually get aligned concurrently is chosen by the gpu hardware. In our
experiments the hardware is able to schedule 4-5 classes per SMP to run
concurrently. This mapping of the problem-level parallelism onto the SMP-level
concurrency is illustrated in Figure \ref{fig:alignment_mapping_gpu}.

\begin{figure}
\centering
\includegraphics[width=3.4in]{figures/alignment_mapping_gpu}
\caption{Proposed mapping of alignment parallelism onto gpu concurrency}
\label{fig:alignment_mapping_gpu}
\end{figure}

In addition, the full resolution query image, along with its numerical derivatives in both
image directions, are stored in GPU texture memory. This  allows the
computation of $b(\tau)$ and $J(\tau)$, which essentially consist of warped
versions of the test image and its derivatives, to exploit the GPU's special
purpose hardware for bilinear interpolation.

In one experiment, we have exploited several possible GPU implementation
strategies, which include solving single alignment problems one at a time using
CUBLAS (302\,ms); solving one problem per SMP using CUDA streams (70\,ms); and
solving one problem per SMP using one large kernel call (36\,ms). The inferior
performance of CUBLAS may be attributed to the relatively small problem size.
In general, the libraries provided by Nvidia are optimized for problem sizes
that are significantly larger than our individual alignment problems.  In
Section \ref{sec:experiment}, we show the performance of our CPU and GPU
implementations running on synthetic and real image data.

\section{Parallelization of the Face Recognition Stage}
\label{sec:recognition}
After the face alignment stage is complete, a single recognition algorithm will
be called to estimate a sparse representation of the query image w.r.t. all the
training classes, as shown in \eqref{eq:l1min_denoise}. In
\cite{WrightJ2009-PAMI,YangA2010-ICIP,WagnerA2011-PAMI}, the recognition
algorithm via $\ell_1$-min has been extensively studied. The algorithm itself
via ALM is summarized in Algorithm \ref{alg:alm_rec}.  In this section, we
mainly focus our discussion on its pixel-level parallelism on the multicore
CPUs and GPUs. Then in Section \ref{sec:experiment}, we benchmark the
performance of the two architectures and further demonstrate the improvement in
both accuracy and speed compared to the existing literature.
\begin{algorithm}[t]
\caption{\bf (Face Recognition via ALM)} \label{alg:alm_rec} 
\begin{algorithmic}[1]
\begin{small}
\STATE {\bf Input:} $\bb \in \Re^m$, $A \in \Re^{m \times n}$,
$\x_1 = \mathbf{0}$, $\e_1 = \bb$, $\y_1 =
\mathbf{0}$.
\WHILE{not converged ($k = 1,2,\ldots$)}
\STATE $\e_{k+1} = \textup{shrink}(\bb - A\x_k +
\frac{1}{\mu}\y_k, \frac{1}{\mu})$;
\STATE $t_1\leftarrow 1$, $\z_1 \leftarrow \x_k$, $\w_1 \leftarrow \x_k$;
\WHILE{not converged ($l = 1,2,\ldots$)}
\STATE $\w_{l+1} \leftarrow \textup{shrink}(\z_l +
\frac{1}{\gamma}A^T(\bb - A\z_l - \e_{k+1} +
\frac{1}{\mu}\y_k), \frac{1}{\mu\gamma})$;
\STATE $t_{l+1} \leftarrow \frac{1}{2}( 1 +
\sqrt{1+4t_l^2})$;
\STATE $\z_{l+1} \leftarrow \w_{l+1} + \frac{t_l - 1}{t_{l+1}}(\w_{l+1} - \w_l)$;
\ENDWHILE
\STATE $\x_{k+1} \leftarrow \w_{l}$,  \; $\y_{k+1} \leftarrow \y_k + \mu (\bb - A\x_{k+1} - \e_{k+1})$;
\ENDWHILE \STATE
{\bf Output:} $\x^* \leftarrow \x_k, \e^* \leftarrow \e_k$.
\end{small}
\end{algorithmic}
\end{algorithm}

\subsection{CPU Implementation} Our CPU implementation of Algorithm
\ref{alg:alm_rec} was written in C++.  For operations in the algorithm that map
nicely onto the BLAS API (such as SGEMV, SAXPY), we
call Intel's MKL BLAS library, which internally takes advantage of the SSE
units on all available CPU cores. 

For operations that do not map nicely onto the BLAS API, such as the shrinkage
operator, {\bf Argue that these parts are optimized, and/or demonstrate that
performance of these parts is insignificant}.

\subsection{GPU Implementation} Our GPU implementation of Algorithm
\ref{alg:alm_rec} is similar to the CPU implementation described in the
previous section, in that only one problem is solved at a time, and each linear
algebraic operation is parallelized separately.

For most of the operations, calls to MKL CPU BLAS library map directly onto
similar calls in Nvidia's GPU BLAS library (CUBLAS).  The CUBLAS library is
designed to utilize all of the SM's on the GPU.  

Operations that do not map well onto the BLAS API (mostly combined
vector-vector operations) were implemented as gpu kernels (in C for CUDA).
Even for some operations that could have been implemented via multiple BLAS
calls, performance improvements were achieved by combining multiple
vector-vector operations into a single kernel, due to reduced bandwidth and
kernel call overhead.  

In order to avoid expensive data transfer across the bus connecting the GPU
card and the CPU motherboard (the PCI express bus), all of the data is
transferred to GPU DRAM once, and all non-trivial tasks are performed on the
GPU on data stored in GPU DRAM.  The sole exception to the above implementation
strategy is that the pseudoinverse of A is still computed on the CPU via the
LAPACK implementation that is part of Intel's MKL. This operation need only be
performed (and results uploaded to GPU DRAM) once before GPU computation
starts. Since the $A$ is very tall, it is only necessary to invert a matrix
of size $n \times n$, which executes very quickly.

\section{Experiments}
\label{sec:experiment}
In this section, we benchmark the performance of our parallel implementations
of $\ell_1$ minimization on CPU and GPU platforms.  In order to show how our
$\ell_1$ minimization algorithms scale with problem size, in Section
\ref{sec:simulation} we will begin with benchmarks on synthetic data.  We will
then progress in Sections \ref{sec:alignment_benchmark} and
\ref{sec:recognition_benchmark} to demonstrating the speed of our
implementations as applied to the alignment and recognition stages of the face
recognition problem.

To generate a meaningful comparison between different hardware architectures,
in all cases we compare the performance of hardware architectures on a
per-board basis:  For CPU implementations, the benchmark makes use of all of
the cores in as many cpu's are present.  For GPU implementations, the benchmark
makes the best use of the entire GPU chip (most GPU boards have a single GPU
chip).  Unless otherwise specified, all implementations utilize single-precision 
floating point datatypes.  

\subsection{Simulations on Random Data}
\label{sec:simulation}

The first experiment compares the performance of our two proposed
implementations of the general $\ell_1$-min (Algorithm \eqref{alg:alm}) given a
generic random matrix $A$ and a random sparse vector $\xx_0$.  The measurement
$d \times n$ matrix $A$ was generated as a random Gaussian matrix, with each
entry randomly generated from the standard normal and normalized with unit
column norm.  The ratio of $d:n$ varied from $1:10$ to $5:10$ with $n$ varying
from 500 to 8000.  The ground truth signals, $\xx_0$ had a sparsity rate of 0.1
with elements generated using the standard normal and also normalized with unit
norm.  Because the ground truth signal $\xx_0$ was known, the algorithm
terminated when $\|\bx-\bx_0\| < \tau$.  The measurement vector was generated
with the equation $\bb_0 = A \xx_0$.   

The results of this benchmark can be found in Figure \ref{fig:random_data}.
The x-axis represents the size of the $A$ matrix and the y-axis represents the
average amount of time it to complete a single problem instance.  The GPU
implementation tends to be faster than the CPU implementation at solving a
single large problem, whereas the the CPU implementation is faster at solving a
single small problem.  Figure \ref{fig:random_data2} shows the various sizes
and tolerances at which our GPU implementation begins to run faster than the
CPU implementation on synthetic data and will provide researchers with a
starting point of where to begin setting their tolerance levels for the ALM
algorithm.
\begin{figure}
\begin{center}
\includegraphics[width=3.5in]{results/random_data/time_vs_matrix_size_constant_tol}
\end{center}
\caption{Speed vs Dimensions of $A$.}
\label{fig:random_data}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=3.5in]{results/random_data/size_vs_speed_crossover_ratio_3}
\end{center}
\caption{Relation between CPU and GPU runtime vs tolerance level}
\label{fig:random_data2}
\end{figure}

\subsection{Face Alignment Stage Benchmark} 
\label{sec:alignment_benchmark}
This section presents benchmarks of our CPU and GPU implementations of the
alignment stage of the face recognition pipeline described in Algorithm
\ref{alg:iterative_alignment}.  The iterative alignment seeks a similarity
transformation between the coordinate frame of the "camera" resolution test
image and the coordinate frame of the "canonical" resolution images which the
images are compared.  We use a $64$ pixel wide square window in the canonical
frame.  The training images are aligned by manually clicking the two outer eye
corners in the camera frame, and then warping the images via a similarity
transformation such that the eye corners lie at a pair of canonical eye
locations in the canonical frame.  The canonical eye corners are chosen such
that for well aligned images, the canonical window contains exclusively pixels
from the subject's face. 

For experiments on face data we use subsets of the CMU Multi-PIE Face Database.
For gallery images we use frontal images from session 1, which contains 20
images per user taken under different illuminations. For test images we use
images from session 2.  

In Figure \ref{fig:alignment_stage_runtime} we report the time it takes
to align a single test image against each of the users in the gallery.
We vary the size of the gallery to show how implementation scales in the
number of gallery users.
\begin{figure}
\centering
\includegraphics[width=3.4in]{figures/alignment_runtime_graph}
\caption{Alignment Stage Runtime vs. Size of Training Database}
\label{fig:alignment_stage_runtime}
\end{figure}

Previous publications used a hard threshold on the number of subjects kept for the recognition
stage as a compromise between recognition speed and potential recognition rate.
In order to better motivate 
a choice for a threshold of the number of users kept for the recognition stage,
we varied the number of users kept for the recognition stage and estimated the probability
of the test user being among the set of training users kept for the recognition stage.
As can be seen in Figure \ref{fig:user_alignment_rank_plot}, there are rapidly diminishing
returns as the number of kept users is increased beyond ten.  
\begin{figure}
\centering
\includegraphics[width=3.4in]{figures/user_alignment_rank_plot}
\caption{Probability of the test user being kept for the recognition stage vs. Number of traning
users kept for the recognition stage}
\label{fig:user_alignment_rank_plot}
\end{figure}

\subsection{Face Recognition Stage Benchmarks}
\label{sec:recognition_benchmark}

\subsection{Face Recognition Pipeline Benchmarks}
\label{sec:pipeline_benchmark}
The fourth experiment compares the recognition rate of the face recognition algorithm at different
image resolutions.  We fixed the number of users to be $enter here$ which was shown in figure  $enter here$  have 95\% probability
to have the true user in the training set, inner loop tolerance to 1e-2 and outer loop tolerance to 1e-3.  

Our final experiment is to compare the speeds of CPU and GPU implementations entire pipeline in Figure \ref{fig:pipeline}.

\section{Conclusion}

{\small
\bibliographystyle{ieee}
\bibliography{faces}
}

\end{document}
