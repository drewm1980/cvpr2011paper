\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{../authorkit/cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Packages added by us
\usepackage{mydefs}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Efficient Parallelization of Sparse Representation for Face Recognition}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\small\url{http://www.author.org/~second}}
}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction} 

Significant strides have been made in automatic face recognition by re-casting
it as a sparse representation problem.  This core of this algorithm consists of
stacking the test image into a vector $\y$, stacking the training images for
all of the users into the columns of a matrix $A$, and solving the following
minimization problem:
\begin{equation}
\min_{\x, \e} \| \x \|_1 + \|\e\|_1 \quad \subj \quad \y = A \x + \e.
\end{equation}
This optimization promotes a solution that is sparse both in $\x$ and $\e$. 
%The solution we are looking for should be sparse in $\x$ because

\cite{Wagner2009-CVPR}
\cite{Wright2009-PAMI}

For iterative alignment, we need to perform the following optimization problem:
\begin{equation}
\min_{\x, \e} \|\e\|_1 \quad \subj \quad \y = A \x + \e.
\end{equation}


\section{Characteristics of the ALM Algorithm}
Describe the two variants of the ALM algorithm

We need to define the following soft-thresholding operator for a
scalar $x$ and a scalar $\alpha \geq 0$:
\begin{equation}
\textup{shrink}(x,\alpha) = \textup{sign}(x)\cdot \max \{|x| - \alpha, 0\},
\end{equation}
In practice, this operator will be applied elementwise to a vector $\x$ with a single $\lambda$,
and will be notated as $\texup{shrink}(\x,\alpha).



Show Pseudocode for one variant
\subsection{Computational Complexity}
The Matrix-vector computions are O($mn$), and the vector-vector operations are O($m$), 
in terms of both computation cost and memory-bandwidth.
Therefore, assuming the iterative solver always takes a constant number of iterations,
the computational complexity of the overall algorithm is linear in the problem size.

\section{Efficient Implementation on the CPU}
% With a reshuffling of the order of operations and a change of the termination
% condition, the inner loop takes the form of two matrix-vector multiplications
% and a sequence of vector-vector operations.
\subsection{CPU Hardware Parallelism}
High-level discussion of CPU parallelism, i.e.
thread-level, vector-level, cache size, cache locality
\subsection{Tuning the GEMV operation}
When matrix-vector operations are called sequentially on the same data,
the operation can be blocked so that each thread operates on the same block of data for each operation.

For Single threaded MKL sgemv, Code took          0.0335823 msec per instance to run!

For Automatic multi-threaded MKL sgemv, Code took 0.008179 msec per instance to run!

For Manually multi-threaded MKL sgemv, Code took  0.0061276 msec per instance to run!

\subsection{Tuning the Soft-Thresholding Operator}
...Make sure there are no branches

...Make sure there are no function calls

...Make sure the loop gets automatically vectorized

...Block the computation into work for different cpu cores

\section{Efficient Implementation on the CPU}
\subsection{GPU Hardware Parallelism}
High-level description of GPU parallelism, i.e.

cuda cores, SIMD width, scheduling restrictions, bandwidth
\subsection{Solving a Single L1 minimization}
This section is about Victor's CUBLAS implementations.
Plot Solver Bandwidth in GB/s vs. Problem size in MB

\subsection{Solving multiple L1 minimizations in parallel}
This section is about Drew's kernels for solving multiple L1 minimizations in parllel

How to efficiently perform an L1 minimization on a single core, quote performance on a large batch.

How to solve multiple instances in smaller batches without losing concurrency, using CUDA streams.

...Mention effect of compiler verion on streams implementation


\section{Application to Face Recognition}
\begin{algorithm}[thb]
\caption{\bf (Deformable Sparse Recovery and Classification for
Face Recognition)} \label{alg:deformable-src}
\begin{algorithmic}[1]
\STATE {\bf Input:} Frontal training images $A_i \in \Re^{m\times n_i}, i=1,2,\ldots,K$ for $K$ subjects,  a test image
$\y\in\Re^m$ and a deformation group $T$. 
\STATE
{\bf for} each subject $i$, 
\STATE \hspace{3mm} $\tau^{(0)}
\leftarrow I$. 
\STATE \hspace{3mm} {\bf while} not converged $(j=1,2,\ldots)$ {\bf do} 
\STATE \hspace{6mm}
$\tilde \y(\tau) \leftarrow \frac{\y \circ \tau}{\|\y \circ
\tau\|_2}$; \;\;\; $J \leftarrow  \frac{\partial}{\partial
\tau} \tilde\y(\tau)  \bigr|_{\tau^{(j)}} $;
%\STATE \hspace{6mm} $(\hat \x, \hat \e, \Delta \tau) \leftarrow \left\{\begin{array}{l} \arg \min_{\x,\e,\Delta \tau} \| \e \|_1 \\  \subj \; \y + J \Delta \tau = A_k \x + \e \end{array}\right.$
\STATE \hspace{6mm} $ \Delta \tau =  \arg\min \; \| \e \|_1  \;
\subj \; \tilde \y + J \Delta \tau = A_i \x + \e.$ 
\STATE
\hspace{6mm} $\tau^{(j+1)} \leftarrow \tau^{(j)} + \Delta
\tau$; 
\STATE \hspace{3mm} {\bf end while} \STATE {\bf end} \STATE Keep
the top $S$ candidates $k_1, \ldots, k_S$ with the smallest
residuals $\|\e\|_1$. \STATE Compute an average transformation
$\bar{\tau}$ from $\tau_{k_1}, \tau_{k_2}, \ldots, \tau_{k_S}$.
\STATE Update $\y \leftarrow \y \circ \bar{\tau}$ and $\tau_i
\leftarrow \tau_i \cdot \bar{\tau}^{-1}$ for $i = k_1, \dots,
k_S$. \STATE Set $A \leftarrow \big[ A_{k_1} \circ
\tau_{k_1}^{-1} \mid A_{k_2} \circ \tau_{k_2}^{-1} \mid \dots
\mid A_{k_S} \circ \tau_{k_S}^{-1} \big]$. \STATE Solve the
$\ell^1$-minimization problem:
$$\hat{\x} = \arg \min_{\x, \e} \| \x \|_1 + \|\e\|_1 \;\; \subj \;\; \y = A \x + \e.$$
\STATE Compute residuals $r_i(\y) = \| {\y} - {A}_i \, \delta_i(\hat{\x}) \|_2$ for $i = k_1, \dots, k_S$.
\STATE {\bf Output:} $\mbox{identity}(\y) = \arg\min_i r_i(\y)$.
\end{algorithmic}
\end{algorithm}

We summarize the entire ALM
algorithm as Algorithm~\ref{alg:alm}, where $\gamma$ denotes the
largest eigenvalue of the matrix $A^TA$. For the choice of parameter $\mu$, we take the same strategy as
in \cite{YangJ2009-pp} and set $\mu = 2m / \|\y\|_1$.
\begin{algorithm}[h]
\caption{\bf (Augmented Lagrange Multiplier Method for Global
Recognition)}
\begin{algorithmic}[1]
\STATE {\bf Input:} $\y \in \Re^m$, $A \in \Re^{m \times n}$,
$\x_1 = \mathbf{0}$, $\e_1 = \y$, $\blamda_1 =
\mathbf{0}$.
\WHILE{not converged ($k = 1,2,\ldots$)}
\STATE $\e_{k+1} = \textup{shrink}\left(\y - A\x_k +
\frac{1}{\mu}\blamda_k, \frac{1}{\mu}\right)$;
\STATE $t_1\leftarrow 1$, $\z_1 \leftarrow \x_k$, $\w_1 \leftarrow \x_k$;
\WHILE{not converged ($l = 1,2,\ldots$)}
\STATE $\w_{l+1} \leftarrow \textup{shrink}\left(\z_l +
\frac{1}{\gamma}A^T\left(\y - A\z_l - \e_{k+1} +
\frac{1}{\mu}\blamda_k\right), \frac{1}{\mu\gamma}\right)$;
\STATE $t_{l+1} \leftarrow \frac{1}{2}\left( 1 +
\sqrt{1+4t_l^2}\right)$;
\STATE $\z_{l+1} \leftarrow \w_{l+1} + \frac{t_l - 1}{t_{l+1}}(\w_{l+1} - \w_l)$;
\ENDWHILE
\STATE $\x_{k+1} \leftarrow \w_{l}$;
\STATE $\blamda_{k+1} \leftarrow \blamda_k + \mu (\y - A\x_{k+1} - \e_{k+1})$;
\ENDWHILE \STATE
{\bf Output:} $\x^* \leftarrow \x_k, \e^* \leftarrow \e_k$.
\end{algorithmic}
\label{alg:alm}
\end{algorithm}



\section{Conclusion}
...How lessons from this paper extend (or don't extend) to other similar problems in computer vision

...Reiterate how the appropriate hardware choice depends on the problem size.

{\small
\bibliographystyle{../authorkit/ieee}
\bibliography{faces}
}


\end{document}
