\documentclass[10pt,twocolumn,letterpaper]{article}
% ICCV Rules:
% 8 page limit (6 free + 2 paid)
% 10MB pdf file size limit
% 30MB supplementary materials file size limit (PDF or ZIP)

\usepackage{iccv}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}

% Needed for getting formulas in dia diagrams.
\usepackage{pstricks}
\usepackage{tikz}

\usepackage{mydefs}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{257} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%\title{Accelerating $\ell_1$-Minimization Using Many-Core CPUs/GPUs \\ and Application to Face Recognition
%\title{Efficient Parallelization of Sparse Representation for Face Recognition}
%\thanks{Corresponding author: . This work was partially supported by ARO MURI W911NF-06-1-0076.}}
%\title{Many-Core Parallelization of $\ell_1$-Minimization for Face Recognition
\title{Parallelization of Fast $\ell_1$-Minimization for Face Recognition
%\thanks{Corresponding author: . This work was partially supported by ARO MURI W911NF-06-1-0076.}
}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\small\url{http://www.author.org/~second}}
}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction} 
$\ell_1$-minimization ($\ell_1$-min) has received much attention in recent
years due to important applications in compressive sensing
\cite{BrucksteinA2007} and sparse representation \cite{WrightJ2010-PIEEE}.  
$\ell_1$-min refers to finding the minimum $\ell$-norm solution to an
underdetermined linear system $\bb=A\xx$:
\begin{equation}
\min \|\xx\|_1\quad \mbox{ subj. to }\quad \bb = A\xx.
\label{eq:l1min}
\end{equation}
It is now well established that, under certain conditions
\cite{CandesE2005-IT_1,DonohoD2004}, the minimum $\ell_1$-norm solution is also
the \emph{sparsest} solution to the system \eqref{eq:l1min}.

Among its many applications, $\ell_1$-min has been recently used to reformulate
image-based face recognition as a sparse representation problem
\cite{WrightJ2009-PAMI}.  If we stack the training images of $K$ subject
classes $(A_1\in\Re^{d\times n_1}, \cdots, A_K\in\Re^{d\times n_K})$ into the
columns of a matrix $A = [A_1, \cdots, A_K]\in\Re^{d\times n}$, given a new
query image in the same vector form $\bb\in\Re^d$, \emph{sparsity-based
classification} (SBC) solves the following minimization problem:
\begin{equation}
\min_{\xx, \ee} \| \xx \|_1 + \|\ee\|_1 \quad \subj \quad \bb = A \xx + \ee.
\label{eq:l1min_denoise}
\end{equation}
If the sparsest solutions for $\x$ and $\e$ are recovered, $\ee$ provides a
means to compensate for pixels that are corrupted due to occlusion of some part of the query
image, and the dominant nonzero coefficients of $\xx$ reveal the membership of
$\bb$ based on the training image labels associated with $A$.  Since $A$ ideally
contains many images per user taken under different illuminations, 
$A\x$ also acts as a linear illumination model for the test image $\bb$.

In this paper, we study parallelization of $\ell$-min on many-core CPU and GPU
architectures. Although $\ell_1$-min \eqref{eq:l1min} is a convex
program, conventional algorithms such as interior-point methods
\cite{ChenS2001-SIAM,TibshiraniR1996} are known to suffer from poor scalability
for large-scale applications such as face recognition. Recently, a number of
accelerated algorithms have been proposed that explicitly take advantage of
the special structure of $\ell_1$-min problems
\cite{LorisI2009,YangA2010-ICIP}. We investigate parallelization of a
state-of-the-art $\ell_1$-min solution based on the classical framework of
\emph{augmented Lagrange multiplier} (ALM) \cite{BertsekasD2003,YangA2010-ICIP}.

While solving \eqref{eq:l1min_denoise} can achieve exceptional
recognition accuracy on public databases, its success relies on the assumption
that the query image is well aligned with the training images. In
\cite{WagnerA2009-CVPR}, the framework of SBC was extended to iteratively align
a query image to one subject class. The face alignment algorithm in turn solves
the following minimization problem:
\begin{equation}
\hat{\tau}_i = \arg\min_{\xx, \ee, \tau_i} \|\ee\|_1\quad \mbox{subj. to}\quad \tilde\bb\circ\tau_i = A_i\xx + \ee,
\label{eq:l1min_alignment}
\end{equation}
where $\tau_i\in T$ is in a finite-dimensional group $T$ of transformations
(e.g., affine or homography) acting on the image domain, which aligns $\tilde\bb$
with the training images from the $i$-th class $A_i$. The algorithm 
extends Lucas-Kanade iterative alignment \cite{LucasB1981} to both to use the
$\ell_1$ norm as a robust error function, and also simultaneously 
to estimate the illumination model $A\x$.  
The original solution \cite{WagnerA2009-CVPR} was implemented based 
on standard interior-point
methods, and its speed has been since improved by ALM in a more recent paper
\cite{WagnerA2011-PAMI}. However, primarily due the high per-user cost
of the alignment step, the recognition system was still not fast enough
for real-time recognition against datasets of hundreds of users.
In this paper, in addition to solving the generic
$\ell_1$-min objectives \eqref{eq:l1min} and \eqref{eq:l1min_denoise}, we 
discuss how to properly accelerate the iterative face alignment step
\eqref{eq:l1min_alignment} on multi-core CPUs and GPUs. 

Finally, we present extensive benchmark to compare the performance of the
generic ALM $\ell_1$-min algorithm and the corresponding face recognition
routines on massively parallel CPUs and GPUs.  To this end, the face
recognition system has been fully implemented on a state-of-the-art
workstation, as shown in Figure \ref{fig:pipeline}. 
Since the parallelization of any algorithm
is highly dependent on the target hardware, we will give a brief overview of
the architecture of our test system in detail in Section
\ref{sec:parallelism}.  We will show that a highly accurate and robust face
recognition system can be constructed to recognize face images from a 
database of several hundred subjects quickly enough to be practical for
access control applications. 
Note that for applications targeting thousands of users or more, it would likely be
necessary to distribute the work over a network of machines; this paper
focuses on the (shared memory) parallelism available in a single machine.
The C++ source code of our implementations, which target Nvidia's CUDA GPU architecture
will be made available for evaluation by other researchers.
\begin{figure}
\centering
{\tiny \input{figures/pipeline_simplified.tex}}
\caption{Overview of the face recognition pipeline.}
\label{fig:pipeline}
\end{figure}

\subsection{Literature Review} 
In this section, we briefly review the literature of $\ell_1$-min and existing
works on its parallelization.  Traditionally, $\ell_1$-min (or the so-called
basis pursuit (BP)) has been formulated as a linear program
\cite{ChenS2001-SIAM}. Several variations of the solution are also well known
in optimization, including a noisy approximation via quadratic programming
called the LASSO \cite{TibshiraniR1996} and truncated Newton interior-point
method (TNIPM) \cite{KimS2007}.

One of the drawbacks of most interior-point methods for $\ell_1$-min (including
BP, LASSO, and TNIPM) is that they require the solution sequence to follow an
interior path
%{\bf DW: Following the interior path is what made them faster than
%traditional linear programming techniques that follow the edge of the feasible set;
%does Homotopy actually go outside the feasible set?  Just want to check if this is
%an accurate representation...}
, which is often computationally expensive. To mitigate these
issues, an approach called \emph{Homotopy} has been studied to accelerate the
speed of $\ell_1$-min. Homotopy methods for recovering sparse signals were
first studied in the context of LASSO \cite{OsborneM2000}, which inspired a
solution to the \emph{forward stagewise linear regression} problem called LARS
\cite{EfronB2004}, and eventually led to the corresponding Homotopy algorithms
for BP in \cite{MalioutovD2005,DonohoD2006}.

Although Homotopy can be shown to exactly estimate BP when the solution is
sufficiently sparse \cite{DonohoD2006}, the algorithm still involves 
computationally expensive operations such as matrix-matrix multiplication and 
the solution of least-squares problems with varying $A$ matrices. 
In Section \ref{sec:ALM}, we contend
that ALM is a better choice among Homotopy and other existing $\ell_1$-min
solvers for implementation on many-core CPUs and GPUs. The ALM algorithm belongs to a
category of approximate $\ell_1$-min solutions called \emph{iterative
shrinkage-thresholding} (IST) method \cite{WrightS2008,BeckA2009}.
IST algorithms mainly involves elementary operations such as vector 
algebra and matrix-vector multiplication. Therefore,
when the dimension of the problem becomes high, IST-type algorithms are
particularly suitable for systems in parallel-computing environments. In
\cite{YangA2010-ICIP}, the authors further showed that, compared to other
IST-type algorithms, ALM is able to achieve good estimation accuracy comparable
to BP and Homotopy, while at the same time significantly improve the speed
compared to these exact $\ell_1$-min solutions. Due to these facts, in this
paper, we choose ALM as the core algorithm to implement the complete parallel
face recognition pipeline.

In terms of the past works in parallel $\ell_1$-min, the literature has been
limited, to the best of our knowledge. In \cite{BorghiA2010}, Borghi et al.
developed a special proximal gradient $\ell_1$-min algorithm based on
Moreau-Yosida regularization. In \cite{MurphyM2010}, Murphy et al. presented
parallel implementation of the l1-SPIRIT MRI reconstruction algorithm on NVidia
programmable GPUs.

\section{Augmented Lagrange Multiplier Method}
\label{sec:ALM}

For the completeness of the paper, in this section, we briefly describe the ALM
algorithm for $\ell_1$-min \eqref{eq:l1min} \cite{YangA2010-ICIP} and analyze
its complexity. Parallelization of ALM for face alignment and recognition will
be discussed in Section \ref{sec:alignment} and Section \ref{sec:recognition},
respectively.

Lagrange multiplier methods have been frequently used in convex programming to
eliminate equality constraints via adding a significant penalty term to the
cost function for infeasible points. ALM methods differ from other
penalty-based approaches by simultaneously estimating the optimal solution and
Lagrange multipliers in an iterative fashion.  For $\ell_1$-min, the augmented
Lagrange function is defined as: \begin{equation} L_\mu(\xx,\yy) = \|\xx\|_1 +
\yy^T(\bb - A\xx - \ee) + \frac{\mu}{2} \| \bb-A\xx-\ee \|_2^2, \end{equation}
where $\mu > 0$ is a constant that penalizes infeasibility and $\yy$ is a
vector of lagrange multipliers.

In Lagrange Multiplier Theory, if there exists a Lagrangian $y^*$ that
satisfies the second-order sufficiency conditions for optimality
\cite{BertsekasD2003}, then for a sufficiently large $mu$, the optimal
$\ell_1$-min solution also minimizes
\begin{equation}
\xx^* = \arg \min L_\mu(\xx,\yy^*).
\label{eq:optimal-ALM}
\end{equation}

In practice, the optimal values for the triplet $(\xx^*, \yy^*, \uu)$ are all
unknown. Furthermore, it has been observed that if solving
\eqref{eq:optimal-ALM} with a large initial value of $\mu$ tends to lead to
slower convergence speed \cite{WrightS2008,YangA2010-ICIP}. In
\cite{BertsekasD2003,YangJ2009}, an alternating procedure has been proposed to
iteratively update $\xx$ and $\yy$:
\begin{equation}
\left \{
\begin{array}{lll}
\xx_{k+1} & = & \arg\min_{\xx} \, L_{\mu_k} (\xx,\yy_k)\\
\yy_{k+1} & = & \yy_k + \mu_k (\bb - A\xx_{k+1}) \\
\end{array}
\right . ,
\label{eq:alm}
\end{equation}
where $\left\{\mu_{k}\right\}$ is a monotonically increasing positive sequence.
The iteration terminates when the estimates $(\xx, \yy)$ converges.

Note that in the iterative procedure outlined in \eqref{eq:alm}, the second
step only involves vector algebra and matrix-vector multiplication. Therefore,
the procedure is computationally efficient only if it is easier to minimize the
augmented Lagrange function compared to solving the original problem
\eqref{eq:l1min} directly. In fact, the first step that minimizes $L_{\mu_k}
(\xx,\yy_k)$ assuming $\yy_k$ and $\mu_k$ are fixed can be solved element-wise
iteratively by a soft-thresholding algorithm \cite{WrightS2008,BeckA2009},
whose time complexity is bounded by $O(n^2)$ and can be easily parallelized.
Algorithm \eqref{alg:alm} summarizes the ALM $\ell_1$-min algorithm \footnote{While we
present the ALM algorithm in the primal domain, there also
exists an implementation ALM in the dual domain.}.

 \begin{algorithm}[h]
\caption{Augmented Lagrange Multiplier (ALM)}
{\bf INPUT:} $\bb \in \Re^m$, $A \in \Re^{m \times n}$, $\tau\leftarrow$ largest eigenvalue of $A^TA$, and a constant $\rho>1$.
\begin{algorithmic}[1]
\WHILE{not converged ($k = 1,2,\ldots$)} \STATE $t_1 \leftarrow 1$, $\zz_1
\leftarrow \xx_k$, $\uu_1 \leftarrow \xx_k$ \WHILE{not
converged ($l = 1,2,\ldots$)} \STATE $\uu_{l+1}  \leftarrow
\mathrm{soft}\left(\zz_l - \frac{1}{\tau}A^T\left(A\zz_l - \bb
- \frac{1}{\mu_k}\yy_k\right), \frac{1}{\mu_k\tau}\right)$
\STATE $t_{l+1} \leftarrow \frac{1}{2}\left( 1 +
\sqrt{1+4t_l^2}\right)$ \STATE $\zz_{l+1} \leftarrow \uu_{l+1}
+ \frac{t_l - 1}{t_{l+1}}(\uu_{l+1} - \uu_l)$ \ENDWHILE \STATE$
\xx_{k+1} \leftarrow \uu_{l+1}$ \STATE $\yy_{k+1} \leftarrow
\yy_k + \mu_k (\bb - A\xx_{k+1})$ \STATE $\mu_{k+1} \leftarrow
\rho\cdot\mu_k$ \ENDWHILE 
\end{algorithmic}

{\bf OUTPUT:} $\xx^* \leftarrow \xx_k$.
\label{alg:alm}
\end{algorithm}

\section{Hardware Parallelism} \label{sec:parallelism} 
In this section we discuss the hardware architectures used in this paper. Aside
from the $\ell_1$-min algorithm, our choice of the architectures plays a
significant role in many practical decisions we make in Section
\ref{sec:alignment} and Section \ref{sec:recognition}.  For CPU implementation,
the system makes use of all the cores in as many CPUs are present on one
motherboard.  For GPU implementation, the system makes the best use of a
single many-core Nvidia GPU chip on a PCI card (GPUs are commonly shipped with
a single GPU chip per PCI-express card).  Unless otherwise specified, all
implementations utilize single precision floating point datatypes.  Since on-chip
memory (cache) is often orders of magnitude faster than off-chip memory, the sizes
and speeds of on-chip memory (cache) have big performance implications.  We give
a brief overview of the caches that are available in our target architectures,
and defer discussion of their performance implications to sections
\ref{sec:alignment_implementation_cpu} and
\ref{sec:alignment_implementation_gpu}.

\subsection{CPU Hardware Parallelism} 
The baseline architecture for our experiments is a Linux workstation with two
quad-core Intel Nehalem E5530 processors clocked at 2.4 GHz.  Each processor
has its own memory interface, and is directly interfaced to half of the RAM
installed in the machine.  The amount of RAM installed exceeds the amount used
by the algorithms, and is not an important performance consideration.  

% CPU CACHE DISCUSSION
%2MB = 2**21 bytes
%64*64*4 = 2**14 bytes = 16384 bytes per image
%... so can fit 2**7  = 128 images.
Each core has a private 32\,KB L1 data cache and a private 256\,KB L2 cache.  
Between the 8 cores, this is a total of 2\,MB L2 cache.
For the low resolution images used in sparse representation based
face recognition (on the order of $64\times64$ pixel images), the L2 cache
can in principle hold up to 128 image vectors.
%16MB = 2**24 bytes = 16777216 bytes
%64*64*4 = 2**14 bytes = 16384 bytes per image
%... so can fit 2**10  = 1024 images.
Each processor further has 8MB of L3 cache that is shared by all four cores.
Overall, this means the algorithm has approximately 16MB of on-chip cache
available.  This corresponds to 1024 image vectors.  

With Hyperthreading enabled, each core can support two threads simultaneously.
For floating point instructions, each core also has a vector processing unit
(SSE) capable of performing the same arithmetic operation on four single
precision floating point values simultaneously.  

There are thus two important levels of parallelism that need to be exploited to
efficiently use a modern CPU: core-level parallelism and vector-level
parallelism. CPU tools for exploiting these two levels of parallelism are
relatively mature.  Heavily optimized Basic Linear Algebra Subprograms (BLAS) are
commercially available that make good use of both levels of CPU parallelism.
For operations that do not fit well with the BLAS API, we 
achieve both vector-level and core-level parallelism through a combination of
the OpenMP API \cite{dagum2002openmp} and the automatic vectorization 
facilities of Intel's C++ compiler (ICC)\cite{dulong1999overview}.
% DW: I'm really not sure these citations are even really relevant;
% our target audience will just google icc openmp and get more up-to-date
% info anyway.
\subsection{GPU Hardware Parallelism}
Compared to CPU architectures, GPU architectures dedicate less of their transistors
to cache, and more of them for arithmetic logic. Our target GPU architecture
is the Nvidia Fermi architecture.  CUDA based GPUs are comprised of several 
streaming multiprocessors (SMP) (currently 14 to 16 for a high end GPU).

The GPU executes programs as SIMT (Single Instruction Multiple Threads) in
groups of 32 threads called \emph{warps}.  These warps are then grouped into
higher levels called \emph{thread blocks} and \emph{grids}.  It is impossible
to communicate between SMPs during gpu program execution, so computational tasks
are typically broken up into multiple gpu programs (kernels) that are executed
serially.  

% GPU CACHE DISCUSSION
% for L2 cache
% 768KB = 786432 bytes
% 64*64*4 = 2**14 bytes = 16384 bytes per image
% 48 images
Each SMP contains its own L1-cache, which is divided between hardware managed
and software managed ("shared") memory.  Additionally, all SMPs share a common
L2-cache.  For our system, each SMP has 64 KB of L1 cache with a total of 896
KB of L1 cache/shared memory and 768KB of L2 cache. The relatively small amount
of cache (1/23 as much as CPU L3) on the GPU is ballanced by a significantly
higher bandwidth between the processor chip and off-chip memory (DRAM) compared
to the CPU.  

The GPU has its own memory system, any data the GPU uses must first be
transferred from CPU DRAM to GPU DRAM over PCI-Express.  For our application,
this transfer overhead is amortized over a large amount of computation and is
not a major concern.

Compared to a typical CPU, a GPU has much higher peak computational bandwidth
as well as data bandwith to DRAM.  However, since our application is primarily
of matrix-vector operations and image resampling operations, it has a very low
arithmetic intensity (i.e. we perform a very small number of arithmetic
operations for every number that has to be loaded), and thus performance is
typically constrained by memory (and cache) bandwidth.  At large problem sizes
the CPU is limited by its relatively low bandwidth to DRAM compared to the gpu,
while at small problem sizes the GPU is limited by the relatively high latency
of its access to DRAM. 

\section{Parallelization of the Face Alignment Stage}
\label{sec:alignment}
%{\bf AY: Add an algorithm here describe the alignment block in the pipeline. Explain the algorithm and necessary modification from generic l-1}
This section provides a brief explanation of the iterative alignment stage
of the recognition pipeline.  
The inputs of the algorithm are the test image $f(x,y)$ stacked into an $M$-
dimensional vector $\f$, and the aligned and stacked training images $A_i$ for
each training user $i$. 
The algorithm repeatedly solves a linearized version of the minimization problem
in \eqref{eq:l1min_alignment}.  This involves two computational steps:
the computation of the linearization, and the solution of the minimization problem.

The computation of the linearization is complicated by the need to avoid trivial 
solutions where the algorithm
progressively zooms in on a black pixel of the test image.
This is achieved by minimizing the $\ell_1$ error between the illumination model $Ax$ and the cropped,
resampled, and normalized test image $\tilde\bb$, rather than the un-normalized version as in 
conventional Lucas-Kanade iterative alignment.
This involves a bit of calculus:
\begin{equation}
\frac{\partial}{\partial \tau_p} \frac{\bb}{||\bb||} 
= \frac{1}{||\bb||} \frac{\partial \bb}{\partial \tau_p} - 
\frac{\bb}{||\bb||^3} \bb^T \frac{\partial \bb}{\partial \tau_p}
\end{equation}
If the warping $\tau$ is constrained to be a similarity transform,
it can be paramaterized in such a way that the computation of $\frac{\partial \bb}{\partial \tau_p}$
reduces to several linear vector operations involving pixel coordinates $(u,v)$, $\tau \in \Re^4$,
and the image gradients $\f_x, \f_y \in \Re^M$, the latter of which only has to be computed
once per test image.  The iterative alignment stage is summarized as Algorithm \eqref{alg:iterative_alignment}.
\begin{algorithm}[thb]
\caption{\bf (Iterative Alignment Stage)} \label{alg:iterative_alignment}
\begin{algorithmic}[1]
\begin{small}
\STATE {\bf Input:} Aligned training images $\{A_i \in \Re^{m\times n_i}\}_{i=1}^K$ for $K$ subjects,  a test image
$\f\in\Re^M$ 
%\STATE $\tau^{(0)} \leftarrow \textrm{face detector($\f$)}$. 
\STATE $\f_x, \f_y \in \Re^M \leftarrow \frac{df}{dx}, \frac{df}{dy}$ 
\STATE {\bf for} each gallery subject $i$, 
\STATE \hspace{3mm} {\bf while} not converged $(j=1,2,\ldots)$ {\bf do} 
\STATE \hspace{6mm} $\tilde \bb(\tau) \leftarrow \frac{\bb \circ \tau}{\|\bb \circ \tau\|_2}$;
\STATE \hspace{6mm} $J \leftarrow  \frac{\partial}{\partial \tau} \tilde\bb(\tau)  \bigr|_{\tau^{(j)}} $;
\STATE \hspace{6mm} $\Delta \tau =  \arg\min \; \| \e \|_1  \; \subj \; \tilde \bb + J \Delta \tau = A_i \x + \e.$ 
\STATE \hspace{6mm} $\tau^{(j+1)} \leftarrow \tau^{(j)} + \Delta
\tau$; 
\end{small}
\end{algorithmic}
\end{algorithm}

The solution of the minimization problem in Algorithm
\eqref{alg:iterative_alignment} Step 7 can be solved via a
variant of ALM, which is summarized as Algorithm \eqref{alg:alignment_alm}.
$\gamma$ denotes the largest eigenvalue of the matrix $A^TA$. For the choice of
parameter $\mu$, we take the same strategy as in \cite{YangJ2009-pp} and set
$\mu_0 = 2m / \|\bb\|_1$. We set $\rho=1.5$.

\begin{algorithm}[h]
\caption{\bf (Augmented Lagrange Multiplier Method For Alignment)}
\begin{algorithmic}[1]
\STATE {\bf Input:} $\bb \in \Re^m$, $A \in \Re^{m \times n}$,
$\x_1 = \mathbf{0}$, $\blamda_1 = \mathbf{0}$.
\WHILE{not converged ($k = 1,2,\ldots$)}
\STATE $t_{1} \leftarrow 1, z_{1} \leftarrow \x_{k}, \u_{1} \leftarrow \x_{k}$;
\WHILE{not converged ($l = 1,2,\ldots$)}
\STATE $\e_{l+1} \leftarrow \textup{shrink}\left(\bb - A\x_l + \frac{\blamda_k}{\mu_k}, \frac{1}{\mu_k}\right)$;
\STATE $\x_{l+1} \leftarrow (A^\dagger)^T \left(\bb - \e_{l+1} + \frac{\blamda_k}{\mu_k} \right) $;
\ENDWHILE
\STATE $\blamda_{k+1} \leftarrow \blamda_k + \mu (\bb - A\x_{k+1} - \e_{k+1})$;
\STATE $\mu_{k+1} \leftarrow \rho\mu_k$;
\ENDWHILE \STATE
{\bf Output:} $\x^* \leftarrow \x_k, \e^* \leftarrow \e_k$.
\end{algorithmic}
\label{alg:alignment_alm}
\end{algorithm}
%\STATE Compute residuals $r_i(\bb) = \| {\bb} - {A}_i \, \delta_i(\hat{\x}) \|_2$ for $i = k_1, \dots, k_S$.

\subsection{Computational Complexity} Any discussion of the speed of an
algorithm should discuss the comutational complexity of the algorithm.  For the
alignment stage ALM algorithm \eqref{alg:alignment_alm}, this is rather simple.
The Matrix-vector computions are $O(mn)$, and the vector-vector operations are
$O(m)$, in terms of both computation cost and memory-bandwidth.  Since A is
very tall, the computation of $A^\dagger$, the pseudoinverse of $A$, is best
computed via $A^\dagger = (A^TA)^{-1} A^T$.  The complexity of computing $G =
A^T A$ is $O(nnm)$.  $G$ is sufficiently small that it can be inverted via
Gauss-Jordan elimination, which has a complexity of $O(n^3)$.  The computation
of $A$ finishes with a matrix multiplication, with cost $O(n^2 m)$.
Fortunately, $A^\dagger$ only needs to be computed once per instance of the
$\ell_1$ minimization problem.

Therefore, for problems with large $n$, we expect the computation of the
pseudoinverse to dominate.  However, for the problem sizes needed in face
recognition problems, we find that the computation time is dominated by the
bandwidth required by repeated computation of matrix-vector and matrix-matrix
multiplications in the inner loop.

I summary, the alignment stage essentially two levels of available parallelism:
at the higher level there are multiple alignent problems that are solved
independently.  At a lower level, the first-order linear algebraic operations
exhibit parallelism within image operations, i.e. at a pixel level.

\subsection{CPU Implementation} 
\label{sec:alignment_implementation_cpu}

This section discusses how the iterative alignment problem can be implemented
on CPU hardware.  The implementations presented in \cite{WagnerA2009-CVPR} and
\cite{WagnerA2011-PAMI} were written using the OpenCV library.  

OpenCV wraps either the operating system's built-in BLAS library, or Math
Kernel Library (MKL), Intel's BLAS implementation, and thus achieves vector
(i.e. pixel)-level parallelism implicitly.  The authors of
\cite{WagnerA2011-PAMI} used OpenMP to schedule instances of the alignment
problem to the CPU cores in a 1:1 fashion.  With eight alignment problems for
ten training users being solved concurrently, and with 20-40 images per
training user, all of the required data fits into L3 cache.  This provided a
significant, though sub-linear improvement over a single-core implementation.

During the course of development, many implementation strategies were tried.
Listed in order of increasing performance based on a synthetic benchmark of
the $\ell_1$ minimization step, they included:
\begin{enumerate}
\item A pure C++ implementation threaded and vectorized with OpenMP and icpc (200 ms).
\item An OpenCV (with MKL backend) implementation threaded with OpenMP similar to \cite{WagnerA2011-PAMI} (190ms.)
\item A straightforward MATLAB implementation (which uses threaded MKL internally) (60 ms.)
\end{enumerate}

In the proposed CPU implementation of the iterative alignment stage, we
therefore perform parallelization in a manner similar to the matlab code:
solving a single problem instance at a time, relying on the threaded version of
MKL BLAS for both multi-core and vector parallelism.  Why is this
implementation faster?  Recall that the L2 cache can old up to 128 image
vectors' worth of data.  Since we're only aligning data for 1 gallery user at a
time the working set of the problem fits into L2 cache, resulting in
significantly higher image alignment speed than previous cpu implementations. 

%\subsection{Tuning the GEMV operation}
%When matrix-vector operations are called sequentially on the same data,
%the operation can be blocked so that each thread operates on the same block of data for each operation.
%For Single threaded MKL sgemv, Code took          0.0335823 msec per instance to run!
%For Automatic multi-threaded MKL sgemv, Code took 0.008179 msec per instance to run!
%For Manually multi-threaded MKL sgemv, Code took  0.0061276 msec per instance to run!

%\subsection{Tuning the Soft-Thresholding Operator}
%...Make sure there are no branches
%...Make sure there are no function calls
%...Make sure the loop gets automatically vectorized
%...Block the computation into work for different cpu cores

\subsection{GPU Implementation}
\label{sec:alignment_implementation_gpu}

This section discusses how to implement algorithm
\eqref{alg:iterative_alignment} on the GPU.  Like for the CPU case, many GPU
implementation strategies were explored during the course of our development,
and are only mentioned briefly.  Listed in order of increasing performance
based on a synthetic benchmark of the $\ell_1$ minimization step, they
included:
\begin{enumerate}
\item Solving a single problem instance at a time using Cublas\footnote{Nvidia provided BLAS implementation} (302 ms)
\item Pure CUDA C implementation solving one problem per SMP using cuda streams\footnote{Streams is a CUDA feature that allows multiple
kernels to be run concurrently} (70 ms)
\item Pure CUDA C implementation solving one problem per SMP using one large kernel call (36 ms)
\end{enumerate}

We attribute the low performance of Cublas in this benchmark to the relatively
low problem size. Cublas is intended to take advantage of the resources of the
entire gpu to solve a single large problem, and our problem size is too small
to hide the memory latency.  Effects of this will be seen again in the
benchmarks in section \ref{sec:simulation}.

One possible implementation of the iterative alignment stage would be to
perform the optimization for all instances simultaneously, but to perform each
linear algebraic operation in a separate gpu kernel call.  The primary
advantage of this implementation strategy is that every kernel would be
performing a very simple operation requiring fewer registers per thread.  The
registers in a GPU are shared between all executing threads thus, barring other
hardware limitations, the fewer registers each thread requires, the more
threads can be scheduled to run simultaneously.  The primary downside of this
approach is that it forces synchronization between all problem instances.  It will
also have increased kernel launch overhead and potentially less efficient use of
the GPU's L2 cache.
% DREW Idea... in hindsight, maybe we should still try many kernel launches. 
% instances can still exit early, and can still update a global variable signaling
% that they're done

Our proposed GPU implementation for the iterative alignment stage consists of a
single kernel that performs all of the test image alignment problems.  Each of
instance of the alignment problem is assigned its own thread block, and the GPU
hardware schedules as many of these thread blocks to run concurrently as the 
hardware will allow.  

The full resolution test image, along with its numerical derivatives in both
image directions, are stored in GPU texture memory. This  allows the
recomputation of $b(\tau)$ and $J(\tau)$, which essentially consist of warped
versions of the test image and its derivatives, to exploit the GPU's special
purpose hardware for bilinear interpolation.

% Should really verify somehow that we're maxing out GPU bandwidth

\section{Parallelization of the Face Recognition Stage}
\label{sec:recognition}
{\bf AY: Add an algorithm here describe the recognition block in the pipeline. Explain the algorithm and necessary modification from generic l-1}
{\bf VS: Why did we modify the algorithm for the recognition stage? }

Below is the algorithm for the face recognition stage in Figure \ref{fig:pipeline}, which
solves the $\ell_1$ minimization problem posed in \eqref{eq:l1min_denoise}. 

\begin{algorithm}[t]
\caption{\bf (Face Recognition via ALM)}
\begin{algorithmic}[1]
\begin{small}
\STATE {\bf Input:} $\bb \in \Re^m$, $A \in \Re^{m \times n}$,
$\x_1 = \mathbf{0}$, $\e_1 = \bb$, $\y_1 =
\mathbf{0}$.
\WHILE{not converged ($k = 1,2,\ldots$)}
\STATE $\e_{k+1} = \textup{shrink}\left(\bb - A\x_k +
\frac{1}{\mu}\y_k, \frac{1}{\mu}\right)$;
\STATE $t_1\leftarrow 1$, $\z_1 \leftarrow \x_k$, $\w_1 \leftarrow \x_k$;
\WHILE{not converged ($l = 1,2,\ldots$)}
\STATE $\w_{l+1} \leftarrow \textup{shrink}\left(\z_l +
\frac{1}{\gamma}A^T\left(\bb - A\z_l - \e_{k+1} +
\frac{1}{\mu}\y_k\right), \frac{1}{\mu\gamma}\right)$;
\STATE $t_{l+1} \leftarrow \frac{1}{2}\left( 1 +
\sqrt{1+4t_l^2}\right)$;
\STATE $\z_{l+1} \leftarrow \w_{l+1} + \frac{t_l - 1}{t_{l+1}}(\w_{l+1} - \w_l)$;
\ENDWHILE
\STATE $\x_{k+1} \leftarrow \w_{l}$,  \; $\y_{k+1} \leftarrow \y_k + \mu (\bb - A\x_{k+1} - \e_{k+1})$;
\ENDWHILE \STATE
{\bf Output:} $\x^* \leftarrow \x_k, \e^* \leftarrow \e_k$.
\end{small}
\end{algorithmic}
\label{alg:alm_rec} 
\end{algorithm}

\subsection{CPU Implementation} Our CPU implementation of Algorithm
\ref{alg:alm_rec} was written in C++.  For operations in the algorithm that map
nicely onto the BLAS API (such as SGEMV, SAXPY {\bf? Victor, fill in here}), we
call Intel's MKL BLAS library, which internally takes advantage of the SSE
units on all available CPU cores. 
{\bf TODO: Our current implementation links against the MKL
library provided internally with matlab; we are in the process of updating the code to link directly against a newer version
of MKL}  

For operations that do not map nicely onto the BLAS API, such as the shrinkage
operator, {\bf Argue that these parts are optimized, and/or demonstrate that
performance of these parts is insignificant}.

\subsection{GPU Implementation} Our GPU implementation of Algorithm
\ref{alg:alm_rec} is similar to the CPU implementation described in the
previous section, in that only one problem is solved at a time, and each linear
algebraic operation is parallelized separately.

For most of the operations, calls to MKL CPU BLAS library map directly onto
similar calls in Nvidia's GPU BLAS library (CUBLAS).  The CUBLAS library is
designed to utilize all of the SM's on the GPU.  

Operations that do not map well onto the BLAS API (mostly combined
vector-vector operations) were implemented as gpu kernels (in C for CUDA).
Even for operations that could have been implemented via multiple BLAS calls,
this was found to have better performance due to reducede bandwidth and kernel
call overhead {\bf Victor, verify or correct this}.

In order to avoid expensive data transfer across the bus connecting the GPU
card and the CPU motherboard (the PCI express bus), all of the data is
transferred to GPU DRAM once, and all non-trivial tasks are performed on the
GPU on data stored in GPU DRAM.  The sole exception to the above implementation
strategy is that the pseudoinverse of A is still computed on the CPU via the
LAPACK implementation that is part of Intel's MKL. This operation need only be
performed (and results uploaded to GPU DRAM) once before GPU computation
starts. {\bf we should prove justification for A pseudoinverse being cheap,
either here or perhaps later in a pie chart}

\section{Experiments}

In this section, we benchmark the performance of our algorithm on CPU and GPU platforms. 
In order to show how our $\ell_1$ minimization algorithms scale with problem size,
we will begin with benchmarks on synthetic data.  We will then progress to demonstrating 
the speed of our implementations as applied to the face recognition problem.

To generate a meaningful comparison between different hardware architectures,
in all cases we compare the performance of hardware architectures on a
per-board basis:  For CPU implementations, the benchmark makes use of all of
the cores in as many cpu's are present.  For GPU implementations, the benchmark
makes the best use of the entire GPU chip (most GPU boards have a single GPU
chip).  Unless otherwise specified, all implementations utilize single
precision floating point datatypes.  

The first experiment is designed to compare the two implementations of PALM
that solve a single large instance of the $\ell_1$-minimization problem (Algorithm \eqref{alg:alm}) by
comparing the amount of time to solve a problem given a generic random matrix $A$ and a random sparse
vector $\xx_0$.  
The measurement $d \times n$ matrix $A$ was generated as a random Gaussian matrix, with each entry
randomly generated from the standard normal and normalized with unit norm.  The ratio of $d:n$ varied from
$10:1$ to $10:5$ with the dimension of $d$ of matrix $A$ varying from 500 to 8000.
The ground truth signals, $\xx_0$ had a sparsity rate of 0.1 with elements generated using the standard normal and normalized with unit norm.
Because the ground truth signal $\xx_0$ is known, the algorithm terminates
when $norm(\bx-\bx_0) < tolerance$.
The measurement vector was generated with the equation $\y_0 = A \x_0$.   
We also varied the different inner and outer tolerance levels from 1e-2 to 1e-5.
We averaged the the time taken for the ALM algorithm to solve 10 instances of a problem.  

%The x set of experiments will compare the CPU and GPU versions of the entire pipeline, 
%which includes the alignment and recognition stage. 

The second experiment is to compare the amount of time to solve the face alignment
stage for each implementation.  Because the alignment stage is not affected by the resolution of the image,
we fix the resolution at 60x80.

Since we use a reduced training set for the recognition stage, our third experiment was to 
find the relationship between the probability that the true subject is in the training set vs the
 number of users in the reduced training set and the speed of the recognition stage w.r.t.
 the number of subjects in the training set.  

The fourth experiment is to compare accuracy rate of the face recognition algorithm at different
image resolutions.  We fixed the number of users to be $enter here$ which was shown in figure  $enter here$  have 95\% probability
to have the true user in the training set, inner loop tolerance to 1e-2 and outer loop tolerance to 1e-3.  

Our final experiment is to compare the speeds of CPU and GPU implementations entire pipeline in Figure \ref{fig:pipeline}.

\subsection{Simulation}
\label{sec:simulation}

We plot the most significant results of the simulation benchmark below.  The x-axis represents the size of the 
$A$ matrix and the y-axis represents the average amount of time it to complete a single problem instance.

\begin{figure}[sim_plot]
\begin{center}
      \includegraphics{../results/random_data/time_vs_matrix_size_constant_tol}
\end{center}
\caption{Speed vs Dimensions of A.  Outer loop tolerance = 0.1.  Inner loop tolerance = $10^{-5}$}
\end{figure}

The observations of this experiment are summarized below:
1.  With similar accuracy rates, the GPU implementation runs faster than the CPU at the crossover points 
noted on the plots.
2.  From the results, it was seen that tolerances...

\subsection{Face Alignment Benchmark}
\label{sec:alignment_benchmark}

\subsection{Face Recognition Benchmarks}
\label{sec:recognition_benchmark}

\subsection{Face Recognition Benchmark}
\label{sec:pipeline_benchmark}

\section{Conclusion}

{\small
\bibliographystyle{ieee}
\bibliography{faces}
}

\end{document}
