\documentclass[10pt,twocolumn,letterpaper]{article}
% ICCV Rules:
% 8 page limit (6 free + 2 paid)
% 10MB pdf file size limit
% 30MB supplementary materials file size limit (PDF or ZIP)

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Packages added by us
\usepackage{amsfonts}
\usepackage{mydefs}
\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


% Needed for getting formulas in dia diagrams.
\usepackage{pstricks}
\usepackage{tikz}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{257} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%\title{Accelerating $\ell_1$-Minimization Using Many-Core CPUs/GPUs \\ and Application to Face Recognition
%\title{Efficient Parallelization of Sparse Representation for Face Recognition}
%\thanks{Corresponding author: . This work was partially supported by ARO MURI W911NF-06-1-0076.}}
%\title{Many-Core Parallelization of $\ell_1$-Minimization for Face Recognition
\title{Parallelization of Fast $\ell_1$-Minimization for Face Recognition
%\thanks{Corresponding author: . This work was partially supported by ARO MURI W911NF-06-1-0076.}
}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\small\url{http://www.author.org/~second}}
}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction} 
$\ell_1$-minimization ($\ell_1$-min) has received much attention in recent
years due to important applications in compressive sensing
\cite{BrucksteinA2007} and sparse representation \cite{WrightJ2010-PIEEE}.  
$\ell_1$-min refers to finding the minimum $\ell$-norm solution to an
underdetermined linear system $\bb=A\xx$:
\begin{equation}
\min \|\xx\|_1\quad \mbox{ subj. to }\quad \bb = A\xx.
\label{eq:l1min}
\end{equation}
It is now well established that, under certain conditions
\cite{CandesE2005-IT_1,DonohoD2004}, the minimum $\ell_1$-norm solution is also
the \emph{sparsest} solution to the system \eqref{eq:l1min}.

Among its many applications, $\ell_1$-min has been recently used to reformulate
image-based face recognition as a sparse representation problem
\cite{WrightJ2009-PAMI}.  If we stack the training images of $K$ subject
classes $(A_1\in\Re^{d\times n_1}, \cdots, A_K\in\Re^{d\times n_K})$ into the
columns of a matrix $A = [A_1, \cdots, A_K]\in\Re^{d\times n}$, given a new
query image in the same vector form $\bb\in\Re^d$, \emph{sparsity-based
classification} (SBC) solves the following minimization problem:
\begin{equation}
\min_{\xx, \ee} \| \xx \|_1 + \|\ee\|_1 \quad \subj \quad \bb = A \xx + \ee.
\label{eq:l1min_denoise}
\end{equation}
If the sparsest solutions for $\x$ and $\e$ are recovered, $\ee$ provides a
means to compensate for pixels that are corrupted due to occlusion of some part of the query
image, and the dominant nonzero coefficients of $\xx$ reveal the membership of
$\bb$ based on the training image labels associated with $A$.  Since $A$ ideally
contains many images per user taken under different illuminations, 
$A\x$ also acts as a linear illumination model for the test image $\bb$.

In this paper, we study parallelization of $\ell$-min on many-core CPU and GPU
architectures. Although $\ell_1$-min \eqref{eq:l1min} is a convex
program, conventional algorithms such as interior-point methods
\cite{ChenS2001-SIAM,TibshiraniR1996} are known to suffer from poor scalability
for large-scale applications such as face recognition. Recently, a number of
accelerated algorithms have been proposed that explicitly take advantage of
the special structure of $\ell_1$-min problems
\cite{LorisI2009,YangA2010-ICIP}. We investigate parallelization of a
state-of-the-art $\ell_1$-min solution based on the classical framework of
\emph{augmented Lagrange multiplier} (ALM) \cite{BertsekasD2003,YangA2010-ICIP}.

While solving \eqref{eq:l1min_denoise} can achieve exceptional
recognition accuracy on public databases, its success relies on the assumption
that the query image is well aligned with the training images. In
\cite{WagnerA2009-CVPR}, the framework of SBC was extended to iteratively align
a query image to one subject class. The face alignment algorithm in turn solves
the following minimization problem:
\begin{equation}
\hat{\tau}_i = \arg\min_{\xx, \ee, \tau_i} \|\ee\|_1\quad \mbox{subj. to}\quad \bb\circ\tau_i = A_i\xx + \ee,
\label{eq:l1min_alignment}
\end{equation}
where $\tau_i\in T$ is in a finite-dimensional group $T$ of transformations
(e.g., affine or homography) acting on the image domain, which aligns $\bb$
with the training images from the $i$-th class $A_i$. The algorithm 
extends Lucas-Kanade iterative alignment \cite{LucasB1981} to both to use the
$\ell_1$ norm as a robust error function, and also simultaneously 
to estimate the illumination model $A\x$.  
The original solution \cite{WagnerA2009-CVPR} was implemented based 
on standard interior-point
methods, and its speed has been since improved by ALM in a more recent paper
\cite{WagnerA2011-PAMI}. However, primarily due the high per-user cost
of the alignment step, the recognition system was still not fast enough
for real-time recognition against datasets of hundreds of users.
In this paper, in addition to solving the generic
$\ell_1$-min objectives \eqref{eq:l1min} and \eqref{eq:l1min_denoise}, we 
discuss how to properly accelerate the iterative face alignment step
\eqref{eq:l1min_alignment} on multi-core CPUs and GPUs. 

Finally, we present extensive benchmark to compare the performance of the
generic ALM $\ell_1$-min algorithm and the corresponding face recognition
routines on massively parallel CPUs and GPUs.  To this end, the face
recognition system has been fully implemented on a state-of-the-art
workstation, as shown in Figure \ref{fig:pipeline}. 
Since the parallelization of any algorithm
is highly dependent on the target hardware, we will give a brief overview of
the architecture of our test system in detail in Section
\ref{sec:parallelism}.  We will show that a highly accurate and robust face
recognition system can be constructed to recognize face images from a 
database of several hundred subjects quickly enough to be practical for
access control applications. 
Note that for applications targeting thousands of users or more, it would likely be
necessary to distribute the work over a network of machines; this paper
focuses on the (shared memory) parallelism available in a single machine.
The C++ source code of our implementations, which target Nvidia's CUDA GPU architecture
will be made available for evaluation by other researchers.
\begin{figure}
\centering
{\tiny \input{figures/pipeline_simplified.tex}}
\caption{Overview of the face recognition pipeline.}
\label{fig:pipeline}
\end{figure}

\subsection{Literature Review} 
In this section, we briefly review the literature of $\ell_1$-min and existing
works on its parallelization.  Traditionally, $\ell_1$-min (or the so-called
basis pursuit (BP)) has been formulated as a linear program
\cite{ChenS2001-SIAM}. Several variations of the solution are also well known
in optimization, including a noisy approximation via quadratic programming
called the LASSO \cite{TibshiraniR1996} and truncated Newton interior-point
method (TNIPM) \cite{KimS2007}.

One of the drawbacks of most interior-point methods for $\ell_1$-min (including
BP, LASSO, and TNIPM) is that they require the solution sequence to follow an
interior path{\bf DW: Following the interior path is what made them faster than
traditional linear programming techniques that follow the edge of the feasible set;
does Homotopy actually go outside the feasible set?  Just want to check if this is
an accurate representation...}, which is often computationally expensive. To mitigate these
issues, an approach called \emph{Homotopy} has been studied to accelerate the
speed of $\ell_1$-min. Homotopy methods for recovering sparse signals were
first studied in the context of LASSO \cite{OsborneM2000}, which inspired a
solution to the \emph{forward stagewise linear regression} problem called LARS
\cite{EfronB2004}, and eventually led to the corresponding Homotopy algorithms
for BP in \cite{MalioutovD2005,DonohoD2006}.

Although Homotopy can be shown to exactly estimate BP when the solution is
sufficiently sparse \cite{DonohoD2006}, the algorithm still involves 
computationally expensive operations such as matrix-matrix multiplication and 
the solution of least-squares problems with varying $A$ matrices. 
In Section \ref{sec:ALM}, we contend
that ALM is a better choice among Homotopy and other existing $\ell_1$-min
solvers for implementation on many-core CPUs and GPUs. The ALM algorithm belongs to a
category of approximate $\ell_1$-min solutions called \emph{iterative
shrinkage-thresholding} (IST) method \cite{WrightS2008,BeckA2009}.
IST algorithms mainly involves elementary operations such as vector 
algebra and matrix-vector multiplication. Therefore,
when the dimension of the problem becomes high, IST-type algorithms are
particularly suitable for systems in parallel-computing environments. In
\cite{YangA2010-ICIP}, the authors further showed that, compared to other
IST-type algorithms, ALM is able to achieve good estimation accuracy comparable
to BP and Homotopy, while at the same time significantly improve the speed
compared to these exact $\ell_1$-min solutions. Due to these facts, in this
paper, we choose ALM as the core algorithm to implement the complete parallel
face recognition pipeline.

In terms of the past works in parallel $\ell_1$-min, the literature has been
limited, to the best of our knowledge. In \cite{BorghiA2010}, Borghi et al.
developed a special proximal gradient $\ell_1$-min algorithm based on
Moreau-Yosida regularization. In \cite{MurphyM2010}, Murphy et al. presented
parallel implementation of the l1-SPIRIT MRI reconstruction algorithm on NVidia
programmable GPUs.

\section{Augmented Lagrange Multiplier Method}
\label{sec:ALM}

For the completeness of the paper, in this section, we briefly describe the ALM
algorithm for $\ell_1$-min \eqref{eq:l1min} \cite{YangA2010-ICIP} and analyze
its complexity. Parallelization of ALM for face alignment and recognition will
be discussed in Section \ref{sec:alignment} and Section \ref{sec:recognition},
respectively.

Lagrange multiplier methods have been frequently used in convex programming to
eliminate equality constraints via adding a significant penalty term to the
cost function for infeasible points. ALM methods differ from other
penalty-based approaches by simultaneously estimating the optimal solution and
Lagrange multipliers in an iterative fashion.  For $\ell_1$-min, the augmented
Lagrange function is defined as: \begin{equation} L_\mu(\xx,\yy) = \|\xx\|_1 +
\yy^T(\bb - A\xx - \ee) + \frac{\mu}{2} \| \bb-A\xx-\ee \|_2^2, \end{equation}
where $\mu > 0$ is a constant that penalizes infeasibility and $\yy$ is a
vector of lagrange multipliers.

In Lagrange Multiplier Theory, if there exists a Lagrangian $y^*$ that
satisfies the second-order sufficiency conditions for optimality
\cite{BertsekasD2003}, then for a sufficiently large $mu$, the optimal
$\ell_1$-min solution also minimizes
\begin{equation}
\xx^* = \arg \min L_\mu(\xx,\yy^*).
\label{eq:optimal-ALM}
\end{equation}

In practice, the optimal values for the triplet $(\xx^*, \yy^*, \uu)$ are all
unknown. Furthermore, it has been observed that if solving
\eqref{eq:optimal-ALM} with a large initial value of $\mu$ tends to lead to
slower convergence speed \cite{WrightS2008,YangA2010-ICIP}. In
\cite{BertsekasD2003,YangJ2009}, an alternating procedure has been proposed to
iteratively update $\xx$ and $\yy$:
\begin{equation}
\left \{
\begin{array}{lll}
\xx_{k+1} & = & \arg\min_{\xx} \, L_{\mu_k} (\xx,\yy_k)\\
\yy_{k+1} & = & \yy_k + \mu_k (\bb - A\xx_{k+1}) \\
\end{array}
\right . ,
\label{eq:alm}
\end{equation}
where $\left\{\mu_{k}\right\}$ is a monotonically increasing positive sequence.
The iteration terminates when the estimates $(\xx, \yy)$ converges.

Note that in the iterative procedure outlined in \eqref{eq:alm}, the second
step only involves vector algebra and matrix-vector multiplication. Therefore,
the procedure is computationally efficient only if it is easier to minimize the
augmented Lagrange function compared to solving the original problem
\eqref{eq:l1min} directly. In fact, the first step that minimizes $L_{\mu_k}
(\xx,\yy_k)$ assuming $\yy_k$ and $\mu_k$ are fixed can be solved element-wise
iteratively by a soft-thresholding algorithm \cite{WrightS2008,BeckA2009},
whose time complexity is bounded by $O(n^2)$ and can be easily parallelized.
Algorithm \ref{alg:alm} summarizes the ALM $\ell_1$-min algorithm.

 \begin{algorithm}[h]
\caption{Augmented Lagrange Multiplier (ALM)}
{\bf INPUT:} $\bb \in \Re^m$, $A \in \Re^{m \times n}$, $\tau\leftarrow$ largest eigenvalue of $A^TA$, and a constant $\rho>1$.
\begin{algorithmic}[1]
\WHILE{not converged ($k = 1,2,\ldots$)} \STATE $t_1 \leftarrow 1$, $\zz_1
\leftarrow \xx_k$, $\uu_1 \leftarrow \xx_k$ \WHILE{not
converged ($l = 1,2,\ldots$)} \STATE $\uu_{l+1}  \leftarrow
\mathrm{soft}\left(\zz_l - \frac{1}{\tau}A^T\left(A\zz_l - \bb
- \frac{1}{\mu_k}\yy_k\right), \frac{1}{\mu_k\tau}\right)$
\STATE $t_{l+1} \leftarrow \frac{1}{2}\left( 1 +
\sqrt{1+4t_l^2}\right)$ \STATE $\zz_{l+1} \leftarrow \uu_{l+1}
+ \frac{t_l - 1}{t_{l+1}}(\uu_{l+1} - \uu_l)$ \ENDWHILE \STATE$
\xx_{k+1} \leftarrow \uu_{l+1}$ \STATE $\yy_{k+1} \leftarrow
\yy_k + \mu_k (\bb - A\xx_{k+1})$ \STATE $\mu_{k+1} \leftarrow
\rho\cdot\mu_k$ \ENDWHILE 
\end{algorithmic}

{\bf OUTPUT:} $\xx^* \leftarrow \xx_k$.
\label{alg:alm}
\end{algorithm}

\section{Hardware Parallelism} \label{sec:parallelism} 
In this section we discuss the hardware architectures used in this paper. Aside
from the $\ell_1$-min algorithm, our choice of the architectures plays a
significant role in many practical decisions we make in Section
\ref{sec:alignment} and Section \ref{sec:recognition}.  For CPU implementation,
the system would make use of all the cores in as many CPUs are present on one
motherboard.  For GPU implementation, the system would make the best use of a
single many-core Nvidia GPU chip  on a PCI card (GPUs are commonly shipped with
a single GPU chip per PCI-express card).  Unless otherwise specified, all
implementations utilize single precision floating point datatypes.  

\subsection{CPU Hardware Parallelism} 
The baseline architecture for our experiments is a Linux workstation with two
quad-core Intel Nehalem E5530 processors clocked at 2.4 GHz.  Each processor
has its own memory interface, and is directly connected to half of the RAM
installed in the machine.  The amount of RAM installed exceeds the amount used
by the algorithms, and is not an important performance consideration.  

Each core has a 32\,KB L1 data cache and a 256\,KB L2 cache. With Hyperthreading
enabled, each core can support two threads simultaneously.  For floating point
instructions, each core also has a vector unit capable of performing the same
arithmetic operation on four single precision floating point values
simultaneously.  \footnote{The core also has scalar floating point units, but
some compilers, notably gcc, generate code that uses the vector units
exclusively for floating point instructions.} 

There are thus two important levels of parallelism that need to be exploited to
efficiently use a modern CPU: core-level parallelism and vector-level
parallelism.  For the implementations discussed here, we will primarily use the
OpenMP programming model to express core-level parallelism {\bf AY: What is
OpenMP? citation needed}.  Depending on the section of the code{\bf AY: What is
a section of the code?}, we will achieve vector-level parallelism implicitly
through either the auto-vectorization features of Intel's icc compiler, or
through the use of a highly optimized BLAS library (Intel's MKL). {\bf AY:
citations needed, what is icc, what is the relation between BLAS and MKL? Are
they equivalent? not clear in the context.}

Each processor further has 8MB of L3 cache that is shared by all four cores.
Overall, this means the algorithm has approximately 16MB of on-chip cache
available.  For the low resolution images used in sparse representation based
face recognition, this is enough cache to hold over 800 image-sized variables,
{\bf AY: how low resolution? which sparse representation problem? ``image-sized
variables'' means simply ``image vectors''?} meaning that for small data sets
it is possible to run completely using on-chip cache.

\subsection{GPU Hardware Parallelism}

The ALM algorithm requires many matrix-vector operations, this gives many-core
processors such as GPUs a computational advantage for large problem sizes.
GPUs are comprised of several streaming multiprocessors (SMP) (14 for our
NVIDIA Fermi) which each contains 32 floating-point units.  Each SMP contains
its own L1-cache and shared memory and all SMPs share a common L2-cache.  The
GPU executes programs as SIMT (Single Instruction Multiple Threads) in groups
of 32 threads called \emph{warps}.  These warps are then grouped into higher
levels called \emph{thread blocks} and \emph{grids}. 

Fermi's inverted memory hierarchy (aggregate l1 capacity is greater than l2 capacity) is designed to support massively parallel workloads in which there is more task-private data than shared data, providing an increase in performance and a computational advantage over CPUs.
For our system, each SMP has 64 kb of L1 cache with a total of 896 kb of L1 cache/shared memory and
768kb of L2 cache. However, this increase in performance does not come for
free. As the GPU has its own memory system, any data the GPU uses must be
transferred from RAM to the GPU over PCI-Express, which has limited bandwidth.
However, the cost of these transfers are amortized by doing large amounts of
computation at once, requiring code to be run completely on the GPU.  In 2011, NVIDIA is developing a new ARM-based CPU system, dubbed 
"Project Denver" to fix performance issues such as the limited bandwidth due to PCI-E \cite{Dally-Web}.

In summary, a programmable GPU can be analogous to a multi-core CPU processor,
except with more sophisticated vector processing hardware.  Each thread block
is a virtualized multiprocessor, which each thread as a virtualized scalar
processor.  All of these differences allow GPUs to perform intensive
calculations such as large scale matrix-vector operations at much faster rates than CPUs. 

\section{Parallelization of the Face Alignment Stage}
\label{sec:alignment}
{\bf AY: Add an algorithm here describe the alignment block in the pipeline. Explain the algorithm and necessary modification from generic l-1}

\subsection{CPU Implementation}

{\bf AY: A diagram should be used to illustrate how the jobs are distributed on a CPU architecture.}
%\subsection{Tuning the GEMV operation}
%When matrix-vector operations are called sequentially on the same data,
%the operation can be blocked so that each thread operates on the same block of data for each operation.
%For Single threaded MKL sgemv, Code took          0.0335823 msec per instance to run!
%For Automatic multi-threaded MKL sgemv, Code took 0.008179 msec per instance to run!
%For Manually multi-threaded MKL sgemv, Code took  0.0061276 msec per instance to run!

%\subsection{Tuning the Soft-Thresholding Operator}
%...Make sure there are no branches
%...Make sure there are no function calls
%...Make sure the loop gets automatically vectorized
%...Block the computation into work for different cpu cores
\subsection{GPU Implementation}
{\bf AY: A diagram should be used to illustrate how the jobs are distributed on a CPU architecture.}

\section{Parallelization of the Face Recognition Stage}
\label{sec:recognition}
{\bf AY: Add an algorithm here describe the recognition block in the pipeline. Explain the algorithm and necessary modification from generic l-1}
{\bf VS: Why did we modify the algorithm for the recognition stage? }

Below is the psuedocode solving the minimization problem posed in \eqref{eq:l1min_denoise}. 

\begin{algorithm}[t]
\caption{\bf (Augmented Lagrange Multiplier Method for
Recognition)}
\begin{algorithmic}[1]
\begin{small}
\STATE {\bf Input:} $\bb \in \Re^m$, $A \in \Re^{m \times n}$,
$\x_1 = \mathbf{0}$, $\e_1 = \bb$, $\y_1 =
\mathbf{0}$.
\WHILE{not converged ($k = 1,2,\ldots$)}
\STATE $\e_{k+1} = \textup{shrink}\left(\bb - A\x_k +
\frac{1}{\mu}\y_k, \frac{1}{\mu}\right)$;
\STATE $t_1\leftarrow 1$, $\z_1 \leftarrow \x_k$, $\w_1 \leftarrow \x_k$;
\WHILE{not converged ($l = 1,2,\ldots$)}
\STATE $\w_{l+1} \leftarrow \textup{shrink}\left(\z_l +
\frac{1}{\gamma}A^T\left(\bb - A\z_l - \e_{k+1} +
\frac{1}{\mu}\y_k\right), \frac{1}{\mu\gamma}\right)$;
\STATE $t_{l+1} \leftarrow \frac{1}{2}\left( 1 +
\sqrt{1+4t_l^2}\right)$;
\STATE $\z_{l+1} \leftarrow \w_{l+1} + \frac{t_l - 1}{t_{l+1}}(\w_{l+1} - \w_l)$;
\ENDWHILE
\STATE $\x_{k+1} \leftarrow \w_{l}$,  \; $\y_{k+1} \leftarrow \y_k + \mu (\bb - A\x_{k+1} - \e_{k+1})$;
\ENDWHILE \STATE
{\bf Output:} $\x^* \leftarrow \x_k, \e^* \leftarrow \e_k$.
\end{small}
\end{algorithmic}
\label{alg:alm_rec} 
\end{algorithm}


\subsection{CPU Implementation}
{\bf AY: A diagram should be used to illustrate how the jobs are distributed on a CPU architecture.}
The CPU ALM algorithm was written in C and compiled with Intel's MKL libraries, which provide
automatic parallelization.  

\subsection{GPU Implementation}
{\bf AY: A diagram should be used to illustrate how the jobs are distributed on a GPU architecture.}
The GPU ALM algorithm is written in CUDA and utilizes the CUBLAS libraries.
CUBLAS are the BLAS libraries consisting of all single precision BLAS calls
which are provided by Nvidia and are optimized for GPU performance. In
addition, we coded kernel (GPU) methods for vector-vector operations to replace
operations that would have taken multiple BLAS calls. Because data transfer from the 
RAM to GPU suffers bandwidth limitations, we designed our implementation to transfer the A matrix and measurement vector at once.  
The algorithm is similar to the CPU algorithm except that we had to parallelize the code at an
additional level. The CPU portion of the code serves as the conductor, making
calls to GPU functions and directing the flow of the code while the GPU keeps all of the data and runs the computation.

\section{Experiment}
{\bf AY: Describe the benchmark protocol here. Both for simulation and real face data.}

In this section, we present four experiments to benchmark the performance of our algorithm on CPU and GPU platforms. 
To generate a meaningful comparison between different hardware architectures,
in all cases we compare the performance of hardware architectures on a
per-board basis:  For CPU implementations, the benchmark makes use of all of
the cores in as many cpu's are present.  For GPU implementations, the benchmark
makes the best use of the entire GPU chip (most GPU boards have a single GPU
chip).  Unless otherwise specified, all implementations utilize single
precision floating point datatypes.  

The first experiment is designed to compare the two implementations of PALM
that solve a single large instance of the $\ell_1$-minimization problem (Algorithm \ref{alg:alm_rec}).  
The measurement $d \times n$ matrix $A$ was generated as a random Gaussian matrix, with each entry
randomly generated from the standard normal and normalized with unit norm.  The ratio of $d:n$ varied from
$10:1$ to $10:5$ with the dimension of $d$ of matrix $A$ varying from 500 to 8000.
The ground truth signals, $x_0$ and $y_k$ had a sparsity rate of 0.1 with elements generated using the standard normal and normalized with unit norm.
The measurement vector was generated with the equation $\y_0 = A \x_0 + \y_k$.    We set the number of inner iterations at 50 and outer iterations at 5000.
We also varied the different inner and tolerance levels from 1e-3 to 1e-6.
We averaged the the time taken for the ALM algorithm to solve 10 instances of a problem.  

The x set of experiments will compare the CPU and GPU versions of the entire pipeline, 
which includes the alignment and recognition stage.

\subsection{Simulation}

We plot the resulted of the simulation benchmark below.  The x-axis represents the size of the 
$A$ and $A^TA$ matrix and the y-axis represents the average amount of time it to complete
a single problem instance.

The observations of this experiment are summarized below:
1.  With similar accuracy rates, the GPU implementation runs faster than the CPU when 

\subsection{Face Recognition Benchmark}

\section{Conclusion}

{\small
\bibliographystyle{ieee}
\bibliography{faces}
}

\end{document}
